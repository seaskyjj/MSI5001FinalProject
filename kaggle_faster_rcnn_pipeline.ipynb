{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electrical Component Detection Pipeline\n",
    "\n",
    "This notebook consolidates the refactored Faster R-CNN training and inference workflow into a single place for convenient experimentation on Kaggle. It provides reusable configuration objects, dataset loaders with optional augmentation, detailed metric utilities (including per-class TP/FP/FN and mAP), and helpers for both training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import contextlib\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Mapping, Optional, Sequence, Set, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image as PILImage, ImageDraw, ImageEnhance, ImageFont\n",
    "from torch import Tensor, nn\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.models.detection import (\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "    fasterrcnn_resnet50_fpn_v2,\n",
    ")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "LOGGER = logging.getLogger(\"notebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PRETRAINED_URL = \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\"\n",
    "\n",
    "#0.999 menas class does not exist\n",
    "DEFAULT_CLASS_SCORE_THRESHOLDS = {\n",
    "    3: 0.999,  \n",
    "    6: 0.8,\n",
    "    7: 0.9,\n",
    "    8: 0.999,\n",
    "    12: 0.999,\n",
    "    16: 0.9,\n",
    "    17: 0.999,\n",
    "    20: 0.9,\n",
    "    21: 0.8,\n",
    "    24: 0.999,\n",
    "    25: 0.9,\n",
    "    26: 0.999,\n",
    "    30: 0.9,\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    \"\"\"Configuration describing the dataset layout and metadata.\"\"\"\n",
    "\n",
    "    base_dir: Path = Path(\"data\")\n",
    "    train_split: str = \"train\"\n",
    "    valid_split: str = \"valid\"\n",
    "    test_split: str = \"test\"\n",
    "    image_folder: str = \"images\"\n",
    "    label_folder: str = \"labels\"\n",
    "    num_classes: int = 32\n",
    "    class_names: Tuple[str, ...] = ()\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if not self.class_names:\n",
    "            self.class_names = tuple(f\"class_{idx:02d}\" for idx in range(self.num_classes))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Hyper-parameters and runtime settings for model training.\"\"\"\n",
    "\n",
    "    epochs: int = 20\n",
    "    batch_size: int = 4\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    num_workers: int = 0\n",
    "    amp: bool = True\n",
    "    augmentation: bool = True\n",
    "    mosaic_prob: float = 0.0\n",
    "    mixup_prob: float = 0.0\n",
    "    mixup_alpha: float = 0.4\n",
    "    scale_jitter_min: float = 1.0\n",
    "    scale_jitter_max: float = 1.0\n",
    "    small_object: bool = True\n",
    "    score_threshold: float = 0.6\n",
    "    iou_threshold: float = 0.5\n",
    "    eval_interval: int = 1\n",
    "    seed: int = 2024\n",
    "    output_dir: Path = Path(\"outputs\")\n",
    "    checkpoint_path: Path = Path(\"outputs/best_model.pth\")\n",
    "    pretrained_weights_path: Path = Path(\"weights/fasterrcnn_resnet50_fpn_v2_coco.pth\")\n",
    "    pretrained_weights_url: str = DEFAULT_PRETRAINED_URL\n",
    "    log_every: int = 20\n",
    "    class_score_thresholds: Dict[int, float] = field(\n",
    "        default_factory=lambda: DEFAULT_CLASS_SCORE_THRESHOLDS.copy()\n",
    "    )\n",
    "    exclude_samples: Tuple[str, ...] = tuple()\n",
    "    fp_visual_dir: Optional[Path] = Path(\"outputs/fp_images\")\n",
    "    fp_report_path: Optional[Path] = None\n",
    "    fp_list_path: Optional[Path] = None\n",
    "    fp_classes: Tuple[int, ...] = (16, 30)\n",
    "\n",
    "    def ensure_directories(self) -> None:\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.pretrained_weights_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if self.fp_visual_dir:\n",
    "            self.fp_visual_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceConfig:\n",
    "    \"\"\"Options for running model inference and visualisation.\"\"\"\n",
    "\n",
    "    score_threshold: float = 0.6\n",
    "    max_images: int = 50\n",
    "    output_dir: Path = Path(\"outputs/inference\")\n",
    "    draw_ground_truth: bool = True\n",
    "    class_colors: List[str] = field(default_factory=list)\n",
    "    class_score_thresholds: Dict[int, float] = field(\n",
    "        default_factory=lambda: DEFAULT_CLASS_SCORE_THRESHOLDS.copy()\n",
    "    )\n",
    "\n",
    "    def ensure_directories(self) -> None:\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading and augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class AugmentationParams:\n",
    "    \"\"\"Parameters controlling the dataset level image augmentations.\"\"\"\n",
    "\n",
    "    horizontal_flip_prob: float = 0.5\n",
    "    vertical_flip_prob: float = 0.2\n",
    "    brightness: float = 0.2\n",
    "    contrast: float = 0.2\n",
    "    saturation: float = 0.2\n",
    "    hue: float = 0.02\n",
    "    mosaic_prob: float = 0.0\n",
    "    mixup_prob: float = 0.0\n",
    "    mixup_alpha: float = 0.4\n",
    "    scale_jitter_range: Tuple[float, float] = (1.0, 1.0)\n",
    "\n",
    "\n",
    "def load_image_hwc_uint8(path: Path) -> np.ndarray:\n",
    "    \"\"\"Load an ``.npy`` image stored as HWC and return an ``uint8`` array.\"\"\"\n",
    "    image = np.load(path, allow_pickle=False, mmap_mode=\"r\")\n",
    "\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.float32, copy=False)\n",
    "        vmin, vmax = float(image.min()), float(image.max())\n",
    "        if 0.0 <= vmin and vmax <= 1.0:\n",
    "            image = (image * 255.0).round()\n",
    "        elif -1.0 <= vmin and vmax <= 1.0:\n",
    "            image = ((image + 1.0) * 0.5 * 255.0).round()\n",
    "        image = np.clip(image, 0, 255).astype(np.uint8)\n",
    "\n",
    "    channels = image.shape[2]\n",
    "    if channels == 1:\n",
    "        image = np.repeat(image, 3, axis=2)\n",
    "    elif channels == 4:\n",
    "        image = image[..., :3]\n",
    "    return image\n",
    "\n",
    "\n",
    "class ElectricalComponentsDataset(Dataset):\n",
    "    \"\"\"Dataset of electrical component detections stored as ``.npy`` images and CSV labels.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Path,\n",
    "        split: str,\n",
    "        class_names: Iterable[str],\n",
    "        transform: Optional[AugmentationParams] = None,\n",
    "        use_augmentation: bool = False,\n",
    "        exclude_stems: Optional[Iterable[str]] = None,\n",
    "    ) -> None:\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.class_names = list(class_names)\n",
    "        self.transform_params = transform or AugmentationParams()\n",
    "        self.use_augmentation = use_augmentation\n",
    "\n",
    "        self.image_dir = self.root / split / \"images\"\n",
    "        self.label_dir = self.root / split / \"labels\"\n",
    "\n",
    "        if not self.image_dir.exists():\n",
    "            raise FileNotFoundError(f\"Missing image directory: {self.image_dir}\")\n",
    "        if not self.label_dir.exists():\n",
    "            raise FileNotFoundError(f\"Missing label directory: {self.label_dir}\")\n",
    "\n",
    "        self.image_stems = sorted(p.stem for p in self.label_dir.glob(\"*.csv\"))\n",
    "        if not self.image_stems:\n",
    "            raise RuntimeError(f\"No label files found in {self.label_dir}\")\n",
    "\n",
    "        exclude_set: Set[str] = set()\n",
    "        if exclude_stems:\n",
    "            exclude_set = {Path(stem).stem for stem in exclude_stems}\n",
    "            if exclude_set:\n",
    "                before = len(self.image_stems)\n",
    "                self.image_stems = [stem for stem in self.image_stems if stem not in exclude_set]\n",
    "                removed = before - len(self.image_stems)\n",
    "                if removed:\n",
    "                    LOGGER.info(\n",
    "                        \"Split %s: excluded %d samples based on provided stem filter.\",\n",
    "                        self.split,\n",
    "                        removed,\n",
    "                    )\n",
    "\n",
    "        self.excluded_stems = sorted(exclude_set)\n",
    "\n",
    "        # Pre-load all annotations to reduce I/O during training.\n",
    "        self.annotations: Dict[str, pd.DataFrame] = {\n",
    "            stem: pd.read_csv(self.label_dir / f\"{stem}.csv\") for stem in self.image_stems\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_stems)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        stem = self.image_stems[index]\n",
    "        image, boxes, labels = self._load_raw_sample(stem)\n",
    "\n",
    "        if self.use_augmentation:\n",
    "            image, boxes, labels = self._apply_composite_augmentations(stem, image, boxes, labels)\n",
    "            image, boxes = self._apply_augmentations(image, boxes)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        image_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "        boxes_tensor = (\n",
    "            torch.from_numpy(boxes).float() if boxes.size else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        )\n",
    "        labels_tensor = (\n",
    "            torch.from_numpy(labels).long() if labels.size else torch.zeros((0,), dtype=torch.long)\n",
    "        )\n",
    "\n",
    "        boxes_tensor, labels_tensor = sanitize_boxes_and_labels(\n",
    "            boxes_tensor, labels_tensor, height, width\n",
    "        )\n",
    "\n",
    "        target: Dict[str, torch.Tensor] = {\n",
    "            \"boxes\": boxes_tensor,\n",
    "            \"labels\": labels_tensor,\n",
    "            \"image_id\": torch.tensor(index, dtype=torch.int64),\n",
    "            \"area\": (boxes_tensor[:, 2] - boxes_tensor[:, 0])\n",
    "            * (boxes_tensor[:, 3] - boxes_tensor[:, 1])\n",
    "            if boxes_tensor.numel()\n",
    "            else torch.tensor([], dtype=torch.float32),\n",
    "            \"iscrowd\": torch.zeros((boxes_tensor.shape[0],), dtype=torch.int64),\n",
    "            \"orig_size\": torch.tensor([height, width], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        return image_tensor, target\n",
    "\n",
    "    def _load_raw_sample(self, stem: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        image_path = self.image_dir / f\"{stem}.npy\"\n",
    "        image = load_image_hwc_uint8(image_path)\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        ann = self.annotations[stem]\n",
    "        boxes, labels = self._annotation_to_boxes(ann, width, height)\n",
    "        return image, boxes, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def _annotation_to_boxes(\n",
    "        ann: pd.DataFrame, width: int, height: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if ann.empty:\n",
    "            return np.zeros((0, 4), dtype=np.float32), np.zeros((0,), dtype=np.int64)\n",
    "\n",
    "        x_center = ann[\"x_center\"].to_numpy(dtype=np.float32)\n",
    "        y_center = ann[\"y_center\"].to_numpy(dtype=np.float32)\n",
    "        box_width = ann[\"width\"].to_numpy(dtype=np.float32)\n",
    "        box_height = ann[\"height\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "        # Auto-detect normalised coordinates and scale back to pixel space.\n",
    "        if (\n",
    "            (x_center.size == 0 or float(x_center.max()) <= 1.0)\n",
    "            and (y_center.size == 0 or float(y_center.max()) <= 1.0)\n",
    "            and (box_width.size == 0 or float(box_width.max()) <= 1.0)\n",
    "            and (box_height.size == 0 or float(box_height.max()) <= 1.0)\n",
    "        ):\n",
    "            x_center = x_center * width\n",
    "            y_center = y_center * height\n",
    "            box_width = box_width * width\n",
    "            box_height = box_height * height\n",
    "\n",
    "        x1 = x_center - box_width / 2.0\n",
    "        y1 = y_center - box_height / 2.0\n",
    "        x2 = x_center + box_width / 2.0\n",
    "        y2 = y_center + box_height / 2.0\n",
    "\n",
    "        boxes = np.stack([x1, y1, x2, y2], axis=1).astype(np.float32)\n",
    "        labels = ann[\"class\"].to_numpy(dtype=np.int64)\n",
    "        return boxes, labels\n",
    "\n",
    "    def _apply_composite_augmentations(\n",
    "        self,\n",
    "        stem: str,\n",
    "        image: np.ndarray,\n",
    "        boxes: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        params = self.transform_params\n",
    "\n",
    "        if (\n",
    "            params.mosaic_prob > 0.0\n",
    "            and random.random() < params.mosaic_prob\n",
    "            and len(self.image_stems) >= 4\n",
    "        ):\n",
    "            image, boxes, labels = self._apply_mosaic(stem, image, boxes, labels)\n",
    "\n",
    "        if (\n",
    "            params.mixup_prob > 0.0\n",
    "            and params.mixup_alpha > 0.0\n",
    "            and random.random() < params.mixup_prob\n",
    "        ):\n",
    "            image, boxes, labels = self._apply_mixup(stem, image, boxes, labels)\n",
    "\n",
    "        if params.scale_jitter_range != (1.0, 1.0):\n",
    "            image, boxes = self._apply_scale_jitter(image, boxes, params.scale_jitter_range)\n",
    "\n",
    "        return image, boxes, labels\n",
    "\n",
    "    def _apply_augmentations(\n",
    "        self, image: np.ndarray, boxes: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        params = self.transform_params\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        if boxes.size and random.random() < params.horizontal_flip_prob:\n",
    "            image = np.ascontiguousarray(image[:, ::-1, :])\n",
    "            x1 = width - boxes[:, 2]\n",
    "            x2 = width - boxes[:, 0]\n",
    "            boxes[:, 0], boxes[:, 2] = x1, x2\n",
    "\n",
    "        if boxes.size and random.random() < params.vertical_flip_prob:\n",
    "            image = np.ascontiguousarray(image[::-1, :, :])\n",
    "            y1 = height - boxes[:, 3]\n",
    "            y2 = height - boxes[:, 1]\n",
    "            boxes[:, 1], boxes[:, 3] = y1, y2\n",
    "\n",
    "        if params.brightness or params.contrast or params.saturation or params.hue:\n",
    "            pil = PILImage.fromarray(image)\n",
    "            if params.brightness:\n",
    "                enhancer = ImageEnhance.Brightness(pil)\n",
    "                factor = 1.0 + random.uniform(-params.brightness, params.brightness)\n",
    "                pil = enhancer.enhance(max(0.1, factor))\n",
    "            if params.contrast:\n",
    "                enhancer = ImageEnhance.Contrast(pil)\n",
    "                factor = 1.0 + random.uniform(-params.contrast, params.contrast)\n",
    "                pil = enhancer.enhance(max(0.1, factor))\n",
    "            if params.saturation:\n",
    "                enhancer = ImageEnhance.Color(pil)\n",
    "                factor = 1.0 + random.uniform(-params.saturation, params.saturation)\n",
    "                pil = enhancer.enhance(max(0.1, factor))\n",
    "            if params.hue:\n",
    "                hsv_image = pil.convert(\"HSV\")\n",
    "                h_channel, s_channel, v_channel = hsv_image.split()\n",
    "                hue_array = np.array(h_channel, dtype=np.uint8)\n",
    "                delta = int(params.hue * 255.0 * random.choice([-1, 1]))\n",
    "                hue_adjusted = ((hue_array.astype(int) + delta) % 255).astype(np.uint8)\n",
    "                new_h = PILImage.fromarray(hue_adjusted)\n",
    "                hsv_image = PILImage.merge(\"HSV\", (new_h, s_channel, v_channel))\n",
    "                pil = hsv_image.convert(\"RGB\")\n",
    "            image = np.array(pil)\n",
    "\n",
    "        if boxes.size:\n",
    "            boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, width)\n",
    "            boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, height)\n",
    "        return image, boxes\n",
    "\n",
    "    def _apply_scale_jitter(\n",
    "        self, image: np.ndarray, boxes: np.ndarray, scale_range: Tuple[float, float]\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        min_scale, max_scale = scale_range\n",
    "        if max_scale <= 0 or min_scale <= 0:\n",
    "            return image, boxes\n",
    "\n",
    "        factor = random.uniform(min_scale, max_scale)\n",
    "        if np.isclose(factor, 1.0):\n",
    "            return image, boxes\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        new_height = max(1, int(round(height * factor)))\n",
    "        new_width = max(1, int(round(width * factor)))\n",
    "\n",
    "        pil = PILImage.fromarray(image)\n",
    "        resized = pil.resize((new_width, new_height), resample=PILImage.BILINEAR)\n",
    "        image = np.array(resized)\n",
    "\n",
    "        if boxes.size:\n",
    "            boxes = boxes.copy()\n",
    "            boxes[:, [0, 2]] *= float(new_width) / float(width)\n",
    "            boxes[:, [1, 3]] *= float(new_height) / float(height)\n",
    "        return image, boxes\n",
    "\n",
    "    def _apply_mosaic(\n",
    "        self,\n",
    "        stem: str,\n",
    "        image: np.ndarray,\n",
    "        boxes: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        height, width = image.shape[:2]\n",
    "        candidate_stems = [s for s in self.image_stems if s != stem]\n",
    "        if len(candidate_stems) < 3:\n",
    "            return image, boxes, labels\n",
    "\n",
    "        selected = random.sample(candidate_stems, 3)\n",
    "\n",
    "        images: List[np.ndarray] = [image]\n",
    "        boxes_list: List[np.ndarray] = [boxes]\n",
    "        labels_list: List[np.ndarray] = [labels]\n",
    "\n",
    "        for other_stem in selected:\n",
    "            other_img, other_boxes, other_labels = self._load_raw_sample(other_stem)\n",
    "            other_img, other_boxes = self._resize_like(other_img, other_boxes, width, height)\n",
    "            images.append(other_img)\n",
    "            boxes_list.append(other_boxes)\n",
    "            labels_list.append(other_labels)\n",
    "\n",
    "        canvas = np.zeros((height * 2, width * 2, 3), dtype=np.uint8)\n",
    "        offsets = [(0, 0), (0, width), (height, 0), (height, width)]\n",
    "        combined_boxes: List[np.ndarray] = []\n",
    "        combined_labels: List[np.ndarray] = []\n",
    "\n",
    "        for img, bxs, lbls, (y_off, x_off) in zip(images, boxes_list, labels_list, offsets):\n",
    "            canvas[y_off : y_off + height, x_off : x_off + width] = img\n",
    "            if bxs.size:\n",
    "                shifted = bxs.copy()\n",
    "                shifted[:, [0, 2]] += x_off\n",
    "                shifted[:, [1, 3]] += y_off\n",
    "                combined_boxes.append(shifted)\n",
    "                combined_labels.append(lbls)\n",
    "\n",
    "        if combined_boxes:\n",
    "            boxes = np.concatenate(combined_boxes, axis=0)\n",
    "            labels = np.concatenate(combined_labels, axis=0)\n",
    "        else:\n",
    "            boxes = np.zeros((0, 4), dtype=np.float32)\n",
    "            labels = np.zeros((0,), dtype=np.int64)\n",
    "\n",
    "        crop_x = random.randint(0, width)\n",
    "        crop_y = random.randint(0, height)\n",
    "        canvas = canvas[crop_y : crop_y + height, crop_x : crop_x + width]\n",
    "\n",
    "        if boxes.size:\n",
    "            boxes = boxes.copy()\n",
    "            boxes[:, [0, 2]] -= crop_x\n",
    "            boxes[:, [1, 3]] -= crop_y\n",
    "\n",
    "            keep = (\n",
    "                (boxes[:, 2] > 0)\n",
    "                & (boxes[:, 3] > 0)\n",
    "                & (boxes[:, 0] < width)\n",
    "                & (boxes[:, 1] < height)\n",
    "            )\n",
    "            boxes = boxes[keep]\n",
    "            labels = labels[keep]\n",
    "            boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, width)\n",
    "            boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, height)\n",
    "\n",
    "        return canvas, boxes, labels\n",
    "\n",
    "    def _apply_mixup(\n",
    "        self,\n",
    "        stem: str,\n",
    "        image: np.ndarray,\n",
    "        boxes: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        other_stem = self._sample_alternative_stem(stem)\n",
    "        if other_stem is None:\n",
    "            return image, boxes, labels\n",
    "\n",
    "        other_img, other_boxes, other_labels = self._load_raw_sample(other_stem)\n",
    "        height, width = image.shape[:2]\n",
    "        other_img, other_boxes = self._resize_like(other_img, other_boxes, width, height)\n",
    "\n",
    "        alpha = max(self.transform_params.mixup_alpha, 1e-3)\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        lam = float(np.clip(lam, 0.3, 0.7))\n",
    "\n",
    "        mixed = (\n",
    "            image.astype(np.float32) * lam + other_img.astype(np.float32) * (1.0 - lam)\n",
    "        ).astype(np.uint8)\n",
    "\n",
    "        if boxes.size and other_boxes.size:\n",
    "            boxes = np.concatenate([boxes, other_boxes], axis=0)\n",
    "            labels = np.concatenate([labels, other_labels], axis=0)\n",
    "        elif other_boxes.size:\n",
    "            boxes = other_boxes.copy()\n",
    "            labels = other_labels.copy()\n",
    "\n",
    "        return mixed, boxes, labels\n",
    "\n",
    "    def _sample_alternative_stem(self, current: str) -> Optional[str]:\n",
    "        if len(self.image_stems) <= 1:\n",
    "            return None\n",
    "        candidates = [stem for stem in self.image_stems if stem != current]\n",
    "        if not candidates:\n",
    "            return None\n",
    "        return random.choice(candidates)\n",
    "\n",
    "    @staticmethod\n",
    "    def _resize_like(\n",
    "        image: np.ndarray, boxes: np.ndarray, target_width: int, target_height: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        height, width = image.shape[:2]\n",
    "        if height == target_height and width == target_width:\n",
    "            return image, boxes\n",
    "\n",
    "        pil = PILImage.fromarray(image)\n",
    "        resized = pil.resize((target_width, target_height), resample=PILImage.BILINEAR)\n",
    "        image = np.array(resized)\n",
    "\n",
    "        if boxes.size:\n",
    "            boxes = boxes.copy()\n",
    "            boxes[:, [0, 2]] *= float(target_width) / float(width)\n",
    "            boxes[:, [1, 3]] *= float(target_height) / float(height)\n",
    "        return image, boxes\n",
    "\n",
    "\n",
    "def detection_collate(batch: List[Tuple[torch.Tensor, Dict[str, torch.Tensor]]]):\n",
    "    \"\"\"Collate function for detection datasets returning lists of tensors.\"\"\"\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n",
    "\n",
    "\n",
    "def _safe_worker_count(requested: int) -> int:\n",
    "    cpu_count = os.cpu_count() or 1\n",
    "    if requested <= 0:\n",
    "        return 0\n",
    "    max_workers = max(1, cpu_count - 1)\n",
    "    return min(requested, max_workers)\n",
    "\n",
    "def _running_in_ipython_kernel() -> bool:\n",
    "    \"\"\"Return ``True`` when executing inside an IPython/Jupyter kernel.\"\"\"\n",
    "\n",
    "    try:  # ``IPython`` is an optional dependency in our runtime.\n",
    "        from IPython import get_ipython  # type: ignore\n",
    "    except Exception:  # pragma: no cover - depends on environment\n",
    "        return False\n",
    "\n",
    "    shell = get_ipython()\n",
    "    return bool(shell and getattr(shell, \"kernel\", None))\n",
    "\n",
    "\n",
    "def emit_metric_lines(\n",
    "    lines: Sequence[str],\n",
    "    *,\n",
    "    logger: Optional[logging.Logger] = None,\n",
    "    force_print: Optional[bool] = None,\n",
    ") -> None:\n",
    "    \"\"\"Log metric lines and optionally mirror them with ``print`` output.\"\"\"\n",
    "\n",
    "    if logger is None:\n",
    "        logger = LOGGER\n",
    "\n",
    "    should_print = force_print if force_print is not None else _running_in_ipython_kernel()\n",
    "\n",
    "    for line in lines:\n",
    "        if logger is not None:\n",
    "            logger.info(line)\n",
    "        if should_print:\n",
    "            print(line)\n",
    "\n",
    "\n",
    "def _should_force_single_worker(dataset: Dataset) -> bool:\n",
    "    \"\"\"Determine whether multiprocessing workers should be disabled.\"\"\"\n",
    "\n",
    "    module_name = getattr(dataset.__class__, \"__module__\", \"\")\n",
    "    if module_name in {\"__main__\", \"__mp_main__\", \"builtins\"}:\n",
    "        return True\n",
    "\n",
    "    if module_name.startswith(\"ipykernel\"):  # pragma: no cover - notebook specific\n",
    "        return True\n",
    "\n",
    "    return _running_in_ipython_kernel()\n",
    "\n",
    "\n",
    "def create_data_loaders(\n",
    "    dataset: Dataset,\n",
    "    batch_size: int,\n",
    "    shuffle: bool,\n",
    "    num_workers: int,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Create a :class:`torch.utils.data.DataLoader` with notebook friendly defaults.\"\"\"\n",
    "\n",
    "    worker_count = _safe_worker_count(num_workers)\n",
    "    if worker_count > 0 and _should_force_single_worker(dataset):\n",
    "        LOGGER.info(\n",
    "            \"Detected interactive environment or in-notebook dataset definition; forcing num_workers=0.\"\n",
    "        )\n",
    "        worker_count = 0\n",
    "    loader_kwargs = dict(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        collate_fn=detection_collate,\n",
    "    )\n",
    "\n",
    "    if worker_count > 0:\n",
    "        loader_kwargs[\"num_workers\"] = worker_count\n",
    "        loader_kwargs[\"persistent_workers\"] = True\n",
    "        loader_kwargs[\"multiprocessing_context\"] = mp.get_context(\"spawn\")\n",
    "    else:\n",
    "        loader_kwargs[\"num_workers\"] = 0\n",
    "\n",
    "    try:\n",
    "        return DataLoader(**loader_kwargs)\n",
    "    except (RuntimeError, OSError, AssertionError) as exc:\n",
    "        if worker_count == 0:\n",
    "            raise\n",
    "        warnings.warn(\n",
    "            \"Falling back to num_workers=0 because DataLoader worker initialisation \"\n",
    "            f\"failed with: {exc}\",\n",
    "            RuntimeWarning,\n",
    "        )\n",
    "        LOGGER.warning(\"DataLoader workers failed to start (%s). Using num_workers=0 instead.\", exc)\n",
    "        loader_kwargs.pop(\"persistent_workers\", None)\n",
    "        loader_kwargs.pop(\"multiprocessing_context\", None)\n",
    "        loader_kwargs[\"num_workers\"] = 0\n",
    "        return DataLoader(**loader_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility helpers and metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def sanitize_boxes_and_labels(\n",
    "    boxes: Tensor, labels: Tensor, height: int, width: int, min_size: float = 1.0\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    if boxes.numel() == 0:\n",
    "        return boxes.reshape(0, 4).float(), labels.reshape(0).long()\n",
    "\n",
    "    boxes = boxes.clone()\n",
    "    boxes[:, 0::2] = boxes[:, 0::2].clamp(0, float(width))\n",
    "    boxes[:, 1::2] = boxes[:, 1::2].clamp(0, float(height))\n",
    "\n",
    "    widths = boxes[:, 2] - boxes[:, 0]\n",
    "    heights = boxes[:, 3] - boxes[:, 1]\n",
    "    keep = (widths > min_size) & (heights > min_size)\n",
    "\n",
    "    if keep.sum() == 0:\n",
    "        return boxes.new_zeros((0, 4)), labels.new_zeros((0,), dtype=torch.long)\n",
    "    return boxes[keep].float(), labels[keep].long()\n",
    "\n",
    "\n",
    "def compute_iou_matrix(boxes1: np.ndarray, boxes2: np.ndarray) -> np.ndarray:\n",
    "    if boxes1.size == 0 or boxes2.size == 0:\n",
    "        return np.zeros((boxes1.shape[0], boxes2.shape[0]), dtype=np.float32)\n",
    "\n",
    "    x11, y11, x12, y12 = np.split(boxes1, 4, axis=1)\n",
    "    x21, y21, x22, y22 = np.split(boxes2, 4, axis=1)\n",
    "\n",
    "    inter_x1 = np.maximum(x11, x21.T)\n",
    "    inter_y1 = np.maximum(y11, y21.T)\n",
    "    inter_x2 = np.minimum(x12, x22.T)\n",
    "    inter_y2 = np.minimum(y12, y22.T)\n",
    "\n",
    "    inter_w = np.clip(inter_x2 - inter_x1, a_min=0.0, a_max=None)\n",
    "    inter_h = np.clip(inter_y2 - inter_y1, a_min=0.0, a_max=None)\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    area1 = (x12 - x11) * (y12 - y11)\n",
    "    area2 = (x22 - x21) * (y22 - y21)\n",
    "\n",
    "    union = area1 + area2.T - inter_area\n",
    "    return np.divide(inter_area, union, out=np.zeros_like(inter_area), where=union > 0)\n",
    "\n",
    "\n",
    "def compute_average_precision(recalls: np.ndarray, precisions: np.ndarray) -> float:\n",
    "    if recalls.size == 0 or precisions.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    mrec = np.concatenate(([0.0], recalls, [1.0]))\n",
    "    mpre = np.concatenate(([0.0], precisions, [0.0]))\n",
    "\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n",
    "\n",
    "    recall_points = np.linspace(0, 1, 101)\n",
    "    precision_interp = np.interp(recall_points, mrec, mpre)\n",
    "    return float(np.trapz(precision_interp, recall_points))\n",
    "\n",
    "\n",
    "def accumulate_classification_stats(\n",
    "    predictions: Sequence[Dict[str, np.ndarray]],\n",
    "    targets: Sequence[Dict[str, np.ndarray]],\n",
    "    num_classes: int,\n",
    "    iou_threshold: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[List[float]], List[List[int]], np.ndarray]:\n",
    "    tp = np.zeros(num_classes, dtype=np.int64)\n",
    "    fp = np.zeros(num_classes, dtype=np.int64)\n",
    "    fn = np.zeros(num_classes, dtype=np.int64)\n",
    "    scores: List[List[float]] = [[] for _ in range(num_classes)]\n",
    "    matches: List[List[int]] = [[] for _ in range(num_classes)]\n",
    "    gt_counter = np.zeros(num_classes, dtype=np.int64)\n",
    "\n",
    "    for pred, tgt in zip(predictions, targets):\n",
    "        pred_boxes = pred[\"boxes\"]\n",
    "        pred_scores = pred[\"scores\"]\n",
    "        pred_labels = pred[\"labels\"].astype(np.int64)\n",
    "\n",
    "        gt_boxes = tgt[\"boxes\"]\n",
    "        gt_labels = tgt[\"labels\"].astype(np.int64)\n",
    "\n",
    "        unique_classes = np.unique(np.concatenate((pred_labels, gt_labels)))\n",
    "        for cls in unique_classes:\n",
    "            pb = pred_boxes[pred_labels == cls]\n",
    "            ps = pred_scores[pred_labels == cls]\n",
    "            tb = gt_boxes[gt_labels == cls]\n",
    "            gt_counter[cls] += len(tb)\n",
    "\n",
    "            if len(tb) == 0:\n",
    "                fp[cls] += len(pb)\n",
    "                scores[cls].extend(ps.tolist())\n",
    "                matches[cls].extend([0] * len(pb))\n",
    "                continue\n",
    "\n",
    "            order = np.argsort(-ps)\n",
    "            pb = pb[order]\n",
    "            ps = ps[order]\n",
    "            iou_matrix = compute_iou_matrix(pb, tb)\n",
    "\n",
    "            matched_gt: set[int] = set()\n",
    "            for det_idx, score in enumerate(ps):\n",
    "                if tb.size == 0:\n",
    "                    fp[cls] += 1\n",
    "                    scores[cls].append(float(score))\n",
    "                    matches[cls].append(0)\n",
    "                    continue\n",
    "\n",
    "                best_gt = int(np.argmax(iou_matrix[det_idx]))\n",
    "                best_iou = iou_matrix[det_idx, best_gt]\n",
    "\n",
    "                if best_iou >= iou_threshold and best_gt not in matched_gt:\n",
    "                    tp[cls] += 1\n",
    "                    matched_gt.add(best_gt)\n",
    "                    scores[cls].append(float(score))\n",
    "                    matches[cls].append(1)\n",
    "                else:\n",
    "                    fp[cls] += 1\n",
    "                    scores[cls].append(float(score))\n",
    "                    matches[cls].append(0)\n",
    "\n",
    "            fn[cls] += len(tb) - len(matched_gt)\n",
    "\n",
    "    return tp, fp, fn, scores, matches, gt_counter\n",
    "\n",
    "\n",
    "def compute_detection_metrics(\n",
    "    predictions: Sequence[Dict[str, np.ndarray]],\n",
    "    targets: Sequence[Dict[str, np.ndarray]],\n",
    "    num_classes: int,\n",
    "    iou_threshold: float,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    tp, fp, fn, scores, matches, gt_counter = accumulate_classification_stats(\n",
    "        predictions, targets, num_classes, iou_threshold\n",
    "    )\n",
    "\n",
    "    precision = np.divide(tp, np.clip(tp + fp, a_min=1, a_max=None))\n",
    "    recall = np.divide(tp, np.clip(tp + fn, a_min=1, a_max=None))\n",
    "\n",
    "    ap = np.zeros(num_classes, dtype=np.float32)\n",
    "    for cls in range(num_classes):\n",
    "        if gt_counter[cls] == 0:\n",
    "            ap[cls] = np.nan\n",
    "            continue\n",
    "        if not scores[cls]:\n",
    "            ap[cls] = 0.0\n",
    "            continue\n",
    "\n",
    "        order = np.argsort(-np.asarray(scores[cls]))\n",
    "        match_array = np.asarray(matches[cls], dtype=np.int32)[order]\n",
    "        cumulative_tp = np.cumsum(match_array)\n",
    "        cumulative_fp = np.cumsum(1 - match_array)\n",
    "\n",
    "        recalls = cumulative_tp / gt_counter[cls]\n",
    "        precisions = cumulative_tp / np.maximum(cumulative_tp + cumulative_fp, 1)\n",
    "        ap[cls] = compute_average_precision(recalls, precisions)\n",
    "\n",
    "    valid_ap = ap[np.isfinite(ap)]\n",
    "    map_value = float(valid_ap.mean()) if valid_ap.size else 0.0\n",
    "\n",
    "    return {\n",
    "        \"TP\": tp,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"AP\": ap,\n",
    "        \"mAP\": map_value,\n",
    "        \"gt_counter\": gt_counter,\n",
    "    }\n",
    "\n",
    "\n",
    "class SmoothedValue:\n",
    "    def __init__(self, window_size: int = 20) -> None:\n",
    "        self.window_size = window_size\n",
    "        self.deque: List[float] = []\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value: float) -> None:\n",
    "        if len(self.deque) == self.window_size:\n",
    "            self.total -= self.deque.pop(0)\n",
    "        self.deque.append(value)\n",
    "        self.total += value\n",
    "        self.count += 1\n",
    "\n",
    "    @property\n",
    "    def avg(self) -> float:\n",
    "        if not self.deque:\n",
    "            return 0.0\n",
    "        return self.total / len(self.deque)\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self) -> None:\n",
    "        self.meters: Dict[str, SmoothedValue] = {}\n",
    "\n",
    "    def update(self, **kwargs: float) -> None:\n",
    "        for name, value in kwargs.items():\n",
    "            if name not in self.meters:\n",
    "                self.meters[name] = SmoothedValue()\n",
    "            self.meters[name].update(float(value))\n",
    "\n",
    "    def format(self) -> str:\n",
    "        parts = [f\"{name}: {meter.avg:.4f}\" for name, meter in self.meters.items()]\n",
    "        return \" | \".join(parts)\n",
    "\n",
    "\n",
    "\n",
    "def score_threshold_mask(\n",
    "    scores: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    default_threshold: float,\n",
    "    class_thresholds: Mapping[int, float],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Return a boolean mask keeping predictions that pass per-class thresholds.\"\"\"\n",
    "\n",
    "    if scores.size == 0:\n",
    "        return np.zeros_like(scores, dtype=bool)\n",
    "\n",
    "    thresholds = np.full(scores.shape, default_threshold, dtype=scores.dtype)\n",
    "    if class_thresholds:\n",
    "        labels_int = labels.astype(np.int64, copy=False)\n",
    "        for cls, value in class_thresholds.items():\n",
    "            thresholds[labels_int == int(cls)] = float(value)\n",
    "\n",
    "    return scores >= thresholds\n",
    "\n",
    "\n",
    "def parse_class_threshold_entries(entries: Sequence[str]) -> Dict[int, float]:\n",
    "    \"\"\"Parse `CLS=THRESH` strings into a mapping of per-class thresholds.\"\"\"\n",
    "\n",
    "    thresholds: Dict[int, float] = {}\n",
    "    for entry in entries:\n",
    "        if not entry:\n",
    "            continue\n",
    "\n",
    "        if \"=\" in entry:\n",
    "            key, value = entry.split(\"=\", 1)\n",
    "        elif \":\" in entry:\n",
    "            key, value = entry.split(\":\", 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid class threshold format: {entry!r}\")\n",
    "\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        if not key or not value:\n",
    "            raise ValueError(f\"Invalid class threshold entry: {entry!r}\")\n",
    "\n",
    "        thresholds[int(key)] = float(value)\n",
    "\n",
    "    return thresholds\n",
    "\n",
    "def identify_false_positive_predictions(\n",
    "    prediction: Dict[str, np.ndarray],\n",
    "    target: Dict[str, np.ndarray],\n",
    "    num_classes: int,\n",
    "    iou_threshold: float,\n",
    ") -> List[Dict[str, Union[int, float, List[float]]]]:\n",
    "    \"\"\"Return detailed records for false-positive detections in a single sample.\"\"\"\n",
    "\n",
    "    boxes = np.asarray(prediction.get(\"boxes\", np.empty((0, 4), dtype=np.float32)), dtype=np.float32)\n",
    "    scores = np.asarray(prediction.get(\"scores\", np.empty((0,), dtype=np.float32)), dtype=np.float32)\n",
    "    labels = np.asarray(prediction.get(\"labels\", np.empty((0,), dtype=np.int64)), dtype=np.int64)\n",
    "\n",
    "    gt_boxes = np.asarray(target.get(\"boxes\", np.empty((0, 4), dtype=np.float32)), dtype=np.float32)\n",
    "    gt_labels = np.asarray(target.get(\"labels\", np.empty((0,), dtype=np.int64)), dtype=np.int64)\n",
    "\n",
    "    if boxes.size == 0:\n",
    "        return []\n",
    "\n",
    "    fp_records: List[Dict[str, Union[int, float, List[float]]]] = []\n",
    "    unique_classes = np.unique(labels) if labels.size else np.asarray([], dtype=np.int64)\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        cls = int(cls)\n",
    "        cls_mask = labels == cls\n",
    "        cls_indices = np.nonzero(cls_mask)[0]\n",
    "        if cls_indices.size == 0:\n",
    "            continue\n",
    "\n",
    "        pb = boxes[cls_mask]\n",
    "        ps = scores[cls_mask]\n",
    "        order = np.argsort(-ps)\n",
    "        pb_sorted = pb[order]\n",
    "        ps_sorted = ps[order]\n",
    "        original_indices = cls_indices[order]\n",
    "\n",
    "        tb = gt_boxes[gt_labels == cls]\n",
    "        iou_matrix = compute_iou_matrix(pb_sorted, tb) if tb.size else np.zeros((pb_sorted.shape[0], 0), dtype=np.float32)\n",
    "        matched_gt: set[int] = set()\n",
    "\n",
    "        for rank, (pred_idx, score_value) in enumerate(zip(original_indices, ps_sorted)):\n",
    "            if iou_matrix.shape[1]:\n",
    "                row = iou_matrix[rank]\n",
    "                best_gt = int(row.argmax())\n",
    "                best_iou = float(row[best_gt])\n",
    "            else:\n",
    "                best_gt = -1\n",
    "                best_iou = 0.0\n",
    "\n",
    "            is_true_positive = (\n",
    "                iou_matrix.shape[1] > 0\n",
    "                and best_iou >= iou_threshold\n",
    "                and best_gt not in matched_gt\n",
    "            )\n",
    "\n",
    "            if is_true_positive:\n",
    "                matched_gt.add(best_gt)\n",
    "                continue\n",
    "\n",
    "            fp_records.append(\n",
    "                {\n",
    "                    \"index\": int(pred_idx),\n",
    "                    \"class\": cls,\n",
    "                    \"score\": float(score_value),\n",
    "                    \"best_iou\": best_iou,\n",
    "                    \"box\": boxes[pred_idx].astype(float).tolist(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return fp_records\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_COLORS = [\n",
    "    \"#FF6B6B\",\n",
    "    \"#4ECDC4\",\n",
    "    \"#556270\",\n",
    "    \"#C44D58\",\n",
    "    \"#FFB347\",\n",
    "    \"#6B5B95\",\n",
    "    \"#88B04B\",\n",
    "    \"#92A8D1\",\n",
    "    \"#955251\",\n",
    "    \"#B565A7\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_default_font() -> ImageFont.FreeTypeFont | ImageFont.ImageFont:\n",
    "    try:\n",
    "        return ImageFont.truetype(\"DejaVuSans.ttf\", size=14)\n",
    "    except Exception:\n",
    "        return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def _resolve_class_name(class_names: Sequence[str], label: int) -> str:\n",
    "    if 0 <= label < len(class_names):\n",
    "        name = class_names[label]\n",
    "    else:\n",
    "        name = f\"class_{label:02d}\"\n",
    "    if name.startswith(\"class_\") and name[6:].isdigit():\n",
    "        return f\"class {int(name[6:]):02d}\"\n",
    "    return name\n",
    "\n",
    "\n",
    "def render_detections(\n",
    "    image: np.ndarray,\n",
    "    prediction: Mapping[str, np.ndarray],\n",
    "    target: Mapping[str, np.ndarray] | None,\n",
    "    class_names: Sequence[str],\n",
    "    score_threshold: float,\n",
    "    class_thresholds: Mapping[int, float],\n",
    "    draw_ground_truth: bool,\n",
    ") -> PILImage:\n",
    "    if image.dtype != np.uint8:\n",
    "        image_array = np.clip(image, 0, 255).astype(np.uint8)\n",
    "    else:\n",
    "        image_array = image\n",
    "\n",
    "    pil = PILImage.fromarray(image_array)\n",
    "    draw = ImageDraw.Draw(pil)\n",
    "    font = load_default_font()\n",
    "\n",
    "    boxes = np.asarray(prediction.get(\"boxes\", np.empty((0, 4))), dtype=float)\n",
    "    labels = np.asarray(prediction.get(\"labels\", np.empty((0,), dtype=int)), dtype=int)\n",
    "    scores = np.asarray(prediction.get(\"scores\", np.empty((0,), dtype=float)), dtype=float)\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        threshold = class_thresholds.get(int(label), score_threshold)\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        color = DEFAULT_COLORS[int(label) % len(DEFAULT_COLORS)] if DEFAULT_COLORS else \"#FF6B6B\"\n",
    "        x1, y1, x2, y2 = [float(coord) for coord in box]\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color, width=2)\n",
    "\n",
    "        caption = f\"{_resolve_class_name(class_names, int(label))} {score:.2f}\"\n",
    "        text_width = draw.textlength(caption, font=font)\n",
    "        draw.rectangle([x1, y1 - 16, x1 + text_width + 8, y1], fill=color)\n",
    "        draw.text((x1 + 4, y1 - 14), caption, fill=\"white\", font=font)\n",
    "\n",
    "    if draw_ground_truth and target is not None:\n",
    "        gt_boxes = np.asarray(target.get(\"boxes\", np.empty((0, 4))), dtype=float)\n",
    "        gt_labels = np.asarray(target.get(\"labels\", np.empty((0,), dtype=int)), dtype=int)\n",
    "        for box, label in zip(gt_boxes, gt_labels):\n",
    "            x1, y1, x2, y2 = [float(coord) for coord in box]\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"#FFFFFF\", width=1)\n",
    "            caption = f\"GT {_resolve_class_name(class_names, int(label))}\"\n",
    "            text_width = draw.textlength(caption, font=font)\n",
    "            draw.rectangle([x1, y2, x1 + text_width + 6, y2 + 14], fill=\"#FFFFFF\")\n",
    "            draw.text((x1 + 3, y2), caption, fill=\"black\", font=font)\n",
    "\n",
    "    return pil\n",
    "\n",
    "\n",
    "def save_detection_visual(\n",
    "    image: np.ndarray,\n",
    "    prediction: Mapping[str, np.ndarray],\n",
    "    target: Mapping[str, np.ndarray] | None,\n",
    "    class_names: Sequence[str],\n",
    "    score_threshold: float,\n",
    "    class_thresholds: Mapping[int, float],\n",
    "    draw_ground_truth: bool,\n",
    "    output_path: Path,\n",
    ") -> None:\n",
    "    visual = render_detections(\n",
    "        image,\n",
    "        prediction,\n",
    "        target,\n",
    "        class_names,\n",
    "        score_threshold,\n",
    "        class_thresholds,\n",
    "        draw_ground_truth,\n",
    "    )\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    visual.save(output_path)\n",
    "\n",
    "\n",
    "def write_false_positive_report(\n",
    "    fp_records: Sequence[Dict[str, object]],\n",
    "    report_path: Path,\n",
    "    *,\n",
    "    split: str,\n",
    "    score_threshold: float,\n",
    "    class_score_thresholds: Mapping[int, float],\n",
    "    iou_threshold: float,\n",
    ") -> None:\n",
    "    payload = {\n",
    "        \"split\": split,\n",
    "        \"score_threshold\": score_threshold,\n",
    "        \"class_score_thresholds\": dict(class_score_thresholds),\n",
    "        \"iou_threshold\": iou_threshold,\n",
    "        \"false_positive_images\": list(fp_records),\n",
    "    }\n",
    "    report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    report_path.write_text(json.dumps(payload, indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "def write_false_positive_list(fp_records: Sequence[Dict[str, object]], list_path: Path) -> None:\n",
    "    stems = sorted({str(record[\"image_id\"]) for record in fp_records})\n",
    "    list_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    list_path.write_text(\"\\n\".join(stems) + (\"\\n\" if stems else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_state_dict(model: nn.Module, path: Path) -> None:\n",
    "    try:\n",
    "        torch.save(model.state_dict(), path)\n",
    "        LOGGER.info(\"Saved pretrained weights to %s\", path)\n",
    "    except Exception as exc:\n",
    "        LOGGER.warning(\"Unable to save pretrained weights: %s\", exc)\n",
    "\n",
    "\n",
    "def _load_pretrained_model(train_cfg: TrainingConfig) -> nn.Module:\n",
    "    pretrained_path = train_cfg.pretrained_weights_path\n",
    "    pretrained_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    weights_enum = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "    try:\n",
    "        model = fasterrcnn_resnet50_fpn_v2(weights=weights_enum)\n",
    "        LOGGER.info(\"Loaded torchvision Faster R-CNN weights\")\n",
    "        if not pretrained_path.exists():\n",
    "            _save_state_dict(model, pretrained_path)\n",
    "        return model\n",
    "    except Exception:\n",
    "        LOGGER.warning(\"Falling back to locally saved pretrained detector weights\")\n",
    "        if not pretrained_path.exists():\n",
    "            raise RuntimeError(\n",
    "                \"No pretrained weights available. Download them manually and place them at \"\n",
    "                + str(pretrained_path)\n",
    "            )\n",
    "        state_dict = torch.load(pretrained_path, map_location=\"cpu\")\n",
    "        model = fasterrcnn_resnet50_fpn_v2(weights=None)\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    dataset_cfg: DatasetConfig,\n",
    "    train_cfg: TrainingConfig,\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> nn.Module:\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = _load_pretrained_model(train_cfg)\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    num_classes_with_background = dataset_cfg.num_classes + 1\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(\n",
    "        in_features, num_classes_with_background\n",
    "    )\n",
    "\n",
    "    if train_cfg.small_object:\n",
    "        anchor_generator = AnchorGenerator(\n",
    "            sizes=((16, 32, 64, 128, 256),) * 5,\n",
    "            aspect_ratios=((0.5, 1.0, 2.0),) * 5,\n",
    "        )\n",
    "        model.rpn.anchor_generator = anchor_generator\n",
    "        LOGGER.info(\"Using custom anchor sizes optimised for small objects\")\n",
    "\n",
    "    model.to(device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(targets: List[Dict[str, torch.Tensor]], device: torch.device) -> List[Dict[str, torch.Tensor]]:\n",
    "    return [{k: v.to(device) for k, v in target.items()} for target in targets]\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler: GradScaler,\n",
    "    device: torch.device,\n",
    "    amp: bool,\n",
    "    log_every: int,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    metric_logger = MetricLogger()\n",
    "    progress = tqdm(loader, desc=\"Train\", leave=False)\n",
    "\n",
    "    for step, (images, targets) in enumerate(progress, start=1):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = move_to_device(targets, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        autocast_enabled = amp and device.type == \"cuda\"\n",
    "        autocast_context = (\n",
    "            torch.amp.autocast(device_type=\"cuda\") if autocast_enabled else contextlib.nullcontext()\n",
    "        )\n",
    "        with autocast_context:\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss_dict.values())\n",
    "\n",
    "        if torch.isfinite(loss):\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            LOGGER.warning(\"Skipping step %s due to non-finite loss\", step)\n",
    "            scaler.update()\n",
    "            continue\n",
    "\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        if step % log_every == 0:\n",
    "            progress.set_postfix_str(metric_logger.format())\n",
    "\n",
    "    return metric_logger.meters.get(\"loss\").avg if metric_logger.meters else 0.0\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    dataset_cfg: DatasetConfig,\n",
    "    train_cfg: TrainingConfig,\n",
    "    *,\n",
    "    dataset: ElectricalComponentsDataset | None = None,\n",
    "    collect_details: bool = False,\n",
    ") -> Tuple[Dict[str, torch.Tensor | float | List[float]], List[Dict[str, object]]]:\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    targets_for_eval = []\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    sample_details: List[Dict[str, object]] = []\n",
    "    dataset_ref = dataset if dataset is not None else getattr(loader, \"dataset\", None)\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets_device = move_to_device(targets, device)\n",
    "\n",
    "        model.train()\n",
    "        loss_dict = model(images, targets_device)\n",
    "        total_loss += sum(loss_dict.values()).item()\n",
    "        num_batches += 1\n",
    "        model.eval()\n",
    "\n",
    "        outputs = model(images)\n",
    "        for output, target_device, target_cpu in zip(outputs, targets_device, targets):\n",
    "            scores = output[\"scores\"].detach().cpu().numpy()\n",
    "            labels = output[\"labels\"].detach().cpu().numpy()\n",
    "            keep = score_threshold_mask(\n",
    "                scores,\n",
    "                labels,\n",
    "                train_cfg.score_threshold,\n",
    "                train_cfg.class_score_thresholds,\n",
    "            )\n",
    "            prediction_np = {\n",
    "                \"boxes\": output[\"boxes\"].detach().cpu().numpy()[keep],\n",
    "                \"scores\": scores[keep],\n",
    "                \"labels\": labels[keep],\n",
    "            }\n",
    "            target_np = {\n",
    "                \"boxes\": target_device[\"boxes\"].detach().cpu().numpy(),\n",
    "                \"labels\": target_device[\"labels\"].detach().cpu().numpy(),\n",
    "            }\n",
    "\n",
    "            predictions.append(prediction_np)\n",
    "            targets_for_eval.append(target_np)\n",
    "\n",
    "            if collect_details:\n",
    "                image_identifier = target_cpu.get(\"image_id\", -1)\n",
    "                if isinstance(image_identifier, torch.Tensor):\n",
    "                    image_index = int(image_identifier.item())\n",
    "                else:\n",
    "                    try:\n",
    "                        image_index = int(image_identifier)\n",
    "                    except Exception:\n",
    "                        image_index = -1\n",
    "                if dataset_ref is not None and 0 <= image_index < len(getattr(dataset_ref, \"image_stems\", [])):\n",
    "                    image_id = dataset_ref.image_stems[image_index]\n",
    "                else:\n",
    "                    image_id = f\"{dataset_cfg.valid_split}_{len(sample_details):04d}\"\n",
    "\n",
    "                fp_info = identify_false_positive_predictions(\n",
    "                    prediction_np,\n",
    "                    target_np,\n",
    "                    dataset_cfg.num_classes,\n",
    "                    train_cfg.iou_threshold,\n",
    "                )\n",
    "\n",
    "                sample_details.append(\n",
    "                    {\n",
    "                        \"image_index\": image_index,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"prediction\": prediction_np,\n",
    "                        \"target\": target_np,\n",
    "                        \"false_positives\": fp_info,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    metrics = compute_detection_metrics(\n",
    "        predictions, targets_for_eval, dataset_cfg.num_classes, train_cfg.iou_threshold\n",
    "    )\n",
    "    metrics[\"loss\"] = total_loss / max(num_batches, 1)\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "    return metrics, sample_details\n",
    "\n",
    "\n",
    "def _resolve_class_label(dataset_cfg: DatasetConfig, index: int) -> str:\n",
    "    if index < len(dataset_cfg.class_names):\n",
    "        label = dataset_cfg.class_names[index]\n",
    "    else:\n",
    "        label = f\"class_{index:02d}\"\n",
    "\n",
    "    if label.startswith(\"class_\") and label[6:].isdigit():\n",
    "        return f\"class {int(label[6:]):02d}\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def format_epoch_metrics(\n",
    "    epoch: Optional[int],\n",
    "    train_loss: Optional[float],\n",
    "    metrics: Dict[str, torch.Tensor | float | List[float]],\n",
    "    dataset_cfg: DatasetConfig,\n",
    "    *,\n",
    "    header: Optional[str] = None,\n",
    ") -> List[str]:\n",
    "    lines: List[str] = []\n",
    "\n",
    "    val_loss = float(metrics.get(\"loss\", float(\"nan\")))\n",
    "    map_value = float(metrics.get(\"mAP\", float(\"nan\")))\n",
    "\n",
    "    if header is not None:\n",
    "        summary = header\n",
    "    elif epoch is not None:\n",
    "        summary = f\"Epoch {epoch:02d}\"\n",
    "    else:\n",
    "        summary = \"Metrics\"\n",
    "\n",
    "    if train_loss is not None and np.isfinite(train_loss):\n",
    "        summary += f\" | train loss {train_loss:.4f}\"\n",
    "    if np.isfinite(val_loss):\n",
    "        summary += f\" | val loss {val_loss:.4f}\"\n",
    "    if np.isfinite(map_value):\n",
    "        summary += f\" | mAP {map_value:.4f}\"\n",
    "    lines.append(summary)\n",
    "\n",
    "    precision = np.asarray(metrics.get(\"precision\", []), dtype=float)\n",
    "    recall = np.asarray(metrics.get(\"recall\", []), dtype=float)\n",
    "    tp = np.asarray(metrics.get(\"TP\", []), dtype=int)\n",
    "    fp = np.asarray(metrics.get(\"FP\", []), dtype=int)\n",
    "    fn = np.asarray(metrics.get(\"FN\", []), dtype=int)\n",
    "    ap = np.asarray(metrics.get(\"AP\", []), dtype=float)\n",
    "    gt_counter = np.asarray(metrics.get(\"gt_counter\", np.zeros_like(tp)), dtype=int)\n",
    "\n",
    "    num_classes = min(len(tp), dataset_cfg.num_classes)\n",
    "    for cls_idx in range(num_classes):\n",
    "        gt_value = int(gt_counter[cls_idx]) if gt_counter.size > cls_idx else 0\n",
    "        tp_value = int(tp[cls_idx]) if tp.size > cls_idx else 0\n",
    "        fp_value = int(fp[cls_idx]) if fp.size > cls_idx else 0\n",
    "        fn_value = int(fn[cls_idx]) if fn.size > cls_idx else 0\n",
    "\n",
    "        if gt_value == 0 and tp_value == 0 and fp_value == 0 and fn_value == 0:\n",
    "            continue\n",
    "\n",
    "        label = _resolve_class_label(dataset_cfg, cls_idx)\n",
    "        if precision.size > cls_idx:\n",
    "            p_val = float(np.nan_to_num(precision[cls_idx], nan=0.0))\n",
    "        else:\n",
    "            p_val = 0.0\n",
    "        if recall.size > cls_idx:\n",
    "            r_val = float(np.nan_to_num(recall[cls_idx], nan=0.0))\n",
    "        else:\n",
    "            r_val = 0.0\n",
    "\n",
    "        line = f\"{label} | P={p_val:.3f} R={r_val:.3f}  TP={tp_value} FP={fp_value} FN={fn_value}\"\n",
    "        if ap.size > cls_idx and np.isfinite(ap[cls_idx]):\n",
    "            line += f\" AP={ap[cls_idx]:.3f}\"\n",
    "        lines.append(line)\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def save_checkpoint(model: nn.Module, path: Path) -> None:\n",
    "    torch.save(model.state_dict(), path)\n",
    "    LOGGER.info(\"Saved checkpoint to %s\", path)\n",
    "\n",
    "\n",
    "def export_false_positive_visuals(\n",
    "    *,\n",
    "    dataset: ElectricalComponentsDataset,\n",
    "    dataset_cfg: DatasetConfig,\n",
    "    train_cfg: TrainingConfig,\n",
    "    sample_details: List[Dict[str, object]],\n",
    ") -> List[Dict[str, object]]:\n",
    "    if not train_cfg.fp_visual_dir:\n",
    "        return []\n",
    "\n",
    "    relevant_classes = {int(cls) for cls in train_cfg.fp_classes}\n",
    "    if not relevant_classes:\n",
    "        return []\n",
    "\n",
    "    output_dir = train_cfg.fp_visual_dir\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for existing in output_dir.glob(\"*.png\"):\n",
    "        try:\n",
    "            existing.unlink()\n",
    "        except OSError:\n",
    "            LOGGER.debug(\"Unable to remove previous FP visual %s\", existing)\n",
    "\n",
    "    fp_records: List[Dict[str, object]] = []\n",
    "\n",
    "    for detail in sample_details:\n",
    "        raw_records = detail.get(\"false_positives\", [])\n",
    "        if not raw_records:\n",
    "            continue\n",
    "\n",
    "        filtered = [fp for fp in raw_records if int(fp.get(\"class\", -1)) in relevant_classes]\n",
    "        if not filtered:\n",
    "            continue\n",
    "\n",
    "        image_id = str(detail.get(\"image_id\", detail.get(\"image_index\", \"unknown\")))\n",
    "        try:\n",
    "            image_np, _, _ = dataset._load_raw_sample(image_id)\n",
    "        except Exception as exc:\n",
    "            LOGGER.warning(\"Skipping FP visual for %s due to load error: %s\", image_id, exc)\n",
    "            continue\n",
    "\n",
    "        output_path = output_dir / f\"{image_id}.png\"\n",
    "        save_detection_visual(\n",
    "            image_np,\n",
    "            detail.get(\"prediction\", {}),\n",
    "            detail.get(\"target\", {}),\n",
    "            dataset_cfg.class_names,\n",
    "            train_cfg.score_threshold,\n",
    "            train_cfg.class_score_thresholds,\n",
    "            True,\n",
    "            output_path,\n",
    "        )\n",
    "        fp_records.append({\"image_id\": image_id, \"false_positives\": filtered})\n",
    "\n",
    "    return fp_records\n",
    "\n",
    "\n",
    "def train_pipeline(\n",
    "    dataset_cfg: DatasetConfig,\n",
    "    train_cfg: TrainingConfig,\n",
    "    *,\n",
    "    resume_from: Optional[Path] = None,\n",
    ") -> Tuple[nn.Module, List[Dict[str, float]]]:\n",
    "    train_cfg.ensure_directories()\n",
    "    set_seed(train_cfg.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = build_model(dataset_cfg, train_cfg, device=device)\n",
    "\n",
    "    if resume_from is not None and Path(resume_from).exists():\n",
    "        LOGGER.info(\"Resuming model weights from %s\", resume_from)\n",
    "        state_dict = torch.load(resume_from, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    train_dataset = ElectricalComponentsDataset(\n",
    "        root=dataset_cfg.base_dir,\n",
    "        split=dataset_cfg.train_split,\n",
    "        class_names=dataset_cfg.class_names,\n",
    "        transform=AugmentationParams(\n",
    "            mosaic_prob=train_cfg.mosaic_prob,\n",
    "            mixup_prob=train_cfg.mixup_prob,\n",
    "            mixup_alpha=train_cfg.mixup_alpha,\n",
    "            scale_jitter_range=(train_cfg.scale_jitter_min, train_cfg.scale_jitter_max),\n",
    "        ),\n",
    "        use_augmentation=train_cfg.augmentation,\n",
    "        exclude_stems=train_cfg.exclude_samples,\n",
    "    )\n",
    "    valid_dataset = ElectricalComponentsDataset(\n",
    "        root=dataset_cfg.base_dir,\n",
    "        split=dataset_cfg.valid_split,\n",
    "        class_names=dataset_cfg.class_names,\n",
    "        use_augmentation=False,\n",
    "    )\n",
    "\n",
    "    if train_cfg.exclude_samples:\n",
    "        sample_preview = \", \".join(train_cfg.exclude_samples[:5])\n",
    "        if len(train_cfg.exclude_samples) > 5:\n",
    "            sample_preview += \", ...\"\n",
    "        LOGGER.info(\n",
    "            \"Excluding %d training samples: %s\",\n",
    "            len(train_cfg.exclude_samples),\n",
    "            sample_preview,\n",
    "        )\n",
    "\n",
    "    train_loader = create_data_loaders(\n",
    "        train_dataset,\n",
    "        batch_size=train_cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=train_cfg.num_workers,\n",
    "    )\n",
    "    if train_cfg.num_workers > 1:\n",
    "        valid_workers = max(1, train_cfg.num_workers // 2)\n",
    "    else:\n",
    "        valid_workers = train_cfg.num_workers\n",
    "\n",
    "    valid_loader = create_data_loaders(\n",
    "        valid_dataset,\n",
    "        batch_size=train_cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=valid_workers,\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=train_cfg.learning_rate, weight_decay=train_cfg.weight_decay)\n",
    "    scaler = GradScaler(enabled=train_cfg.amp and device.type == \"cuda\")\n",
    "\n",
    "    best_map = -float(\"inf\")\n",
    "    history: List[Dict[str, float]] = []\n",
    "\n",
    "    for epoch in range(1, train_cfg.epochs + 1):\n",
    "        LOGGER.info(\"Epoch %s/%s\", epoch, train_cfg.epochs)\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, optimizer, scaler, device, train_cfg.amp, train_cfg.log_every\n",
    "        )\n",
    "\n",
    "        should_evaluate = (epoch % train_cfg.eval_interval == 0) or (epoch == train_cfg.epochs)\n",
    "        collect_fp = should_evaluate and epoch == train_cfg.epochs\n",
    "\n",
    "        if should_evaluate:\n",
    "            metrics, sample_details = evaluate(\n",
    "                model,\n",
    "                valid_loader,\n",
    "                device,\n",
    "                dataset_cfg,\n",
    "                train_cfg,\n",
    "                dataset=valid_dataset,\n",
    "                collect_details=collect_fp,\n",
    "            )\n",
    "            metric_lines = format_epoch_metrics(epoch, train_loss, metrics, dataset_cfg)\n",
    "            emit_metric_lines(metric_lines, logger=LOGGER)\n",
    "\n",
    "            history.append(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train_loss\": float(train_loss),\n",
    "                    \"val_loss\": float(metrics[\"loss\"]),\n",
    "                    \"mAP\": float(metrics[\"mAP\"]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if metrics[\"mAP\"] > best_map:\n",
    "                best_map = float(metrics[\"mAP\"])\n",
    "                save_checkpoint(model, train_cfg.checkpoint_path)\n",
    "\n",
    "            if collect_fp:\n",
    "                fp_records = export_false_positive_visuals(\n",
    "                    dataset=valid_dataset,\n",
    "                    dataset_cfg=dataset_cfg,\n",
    "                    train_cfg=train_cfg,\n",
    "                    sample_details=sample_details,\n",
    "                )\n",
    "                if train_cfg.fp_report_path:\n",
    "                    write_false_positive_report(\n",
    "                        fp_records,\n",
    "                        train_cfg.fp_report_path,\n",
    "                        split=dataset_cfg.valid_split,\n",
    "                        score_threshold=train_cfg.score_threshold,\n",
    "                        class_score_thresholds=train_cfg.class_score_thresholds,\n",
    "                        iou_threshold=train_cfg.iou_threshold,\n",
    "                    )\n",
    "                    LOGGER.info(\n",
    "                        \"Saved false-positive report for %d images to %s\",\n",
    "                        len(fp_records),\n",
    "                        train_cfg.fp_report_path,\n",
    "                    )\n",
    "                if train_cfg.fp_list_path:\n",
    "                    write_false_positive_list(fp_records, train_cfg.fp_list_path)\n",
    "                    LOGGER.info(\n",
    "                        \"Saved list of %d false-positive image ids to %s\",\n",
    "                        len({str(record[\"image_id\"]) for record in fp_records}),\n",
    "                        train_cfg.fp_list_path,\n",
    "                    )\n",
    "        else:\n",
    "            LOGGER.info(\n",
    "                \"Epoch %02d | train loss %.4f | evaluation skipped (eval_interval=%d)\",\n",
    "                epoch,\n",
    "                train_loss,\n",
    "                train_cfg.eval_interval,\n",
    "            )\n",
    "\n",
    "    (train_cfg.output_dir / \"training_history.json\").write_text(json.dumps(history, indent=2))\n",
    "    LOGGER.info(\"Training complete. Best mAP: %.4f\", best_map)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_inference(\n",
    "    dataset_cfg: DatasetConfig,\n",
    "    inference_cfg: InferenceConfig,\n",
    "    train_cfg: TrainingConfig,\n",
    "    checkpoint_path: Path,\n",
    "    split: Optional[str] = None,\n",
    "    fp_report_path: Optional[Path] = None,\n",
    "    fp_list_path: Optional[Path] = None,\n",
    ") -> Dict[str, object]:\n",
    "    inference_cfg.ensure_directories()\n",
    "    set_seed(train_cfg.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = build_model(dataset_cfg, train_cfg, device=device)\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    dataset = ElectricalComponentsDataset(\n",
    "        root=dataset_cfg.base_dir,\n",
    "        split=split or dataset_cfg.test_split,\n",
    "        class_names=dataset_cfg.class_names,\n",
    "        use_augmentation=False,\n",
    "    )\n",
    "    loader = create_data_loaders(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    predictions: List[Dict[str, np.ndarray]] = []\n",
    "    targets_for_eval: List[Dict[str, np.ndarray]] = []\n",
    "    fp_records: List[Dict[str, object]] = []\n",
    "\n",
    "    progress = tqdm(loader, desc=\"Infer\")\n",
    "    for idx, (images, targets) in enumerate(progress):\n",
    "        image = images[0].to(device)\n",
    "        output = model([image])[0]\n",
    "\n",
    "        boxes_np = output[\"boxes\"].detach().cpu().numpy()\n",
    "        scores_np = output[\"scores\"].detach().cpu().numpy()\n",
    "        labels_np = output[\"labels\"].detach().cpu().numpy()\n",
    "        keep = score_threshold_mask(\n",
    "            scores_np,\n",
    "            labels_np,\n",
    "            inference_cfg.score_threshold,\n",
    "            inference_cfg.class_score_thresholds,\n",
    "        )\n",
    "        prediction_np = {\n",
    "            \"boxes\": boxes_np[keep],\n",
    "            \"scores\": scores_np[keep],\n",
    "            \"labels\": labels_np[keep],\n",
    "        }\n",
    "        target_np = {\n",
    "            \"boxes\": targets[0][\"boxes\"].detach().cpu().numpy(),\n",
    "            \"labels\": targets[0][\"labels\"].detach().cpu().numpy(),\n",
    "        }\n",
    "\n",
    "        predictions.append(prediction_np)\n",
    "        targets_for_eval.append(target_np)\n",
    "\n",
    "        fp_details = identify_false_positive_predictions(\n",
    "            prediction_np,\n",
    "            target_np,\n",
    "            dataset_cfg.num_classes,\n",
    "            train_cfg.iou_threshold,\n",
    "        )\n",
    "        if fp_details:\n",
    "            default_split = split or dataset_cfg.test_split\n",
    "            image_id = dataset.image_stems[idx] if idx < len(dataset.image_stems) else f\"{default_split}_{idx:04d}\"\n",
    "            fp_records.append(\n",
    "                {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"false_positives\": fp_details,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if idx < inference_cfg.max_images:\n",
    "            image_np = (images[0].permute(1, 2, 0).numpy() * 255.0).astype(np.uint8)\n",
    "            output_path = inference_cfg.output_dir / f\"{(split or dataset_cfg.test_split)}_{idx:04d}.png\"\n",
    "            save_detection_visual(\n",
    "                image_np,\n",
    "                prediction_np,\n",
    "                target_np if inference_cfg.draw_ground_truth else None,\n",
    "                dataset_cfg.class_names,\n",
    "                inference_cfg.score_threshold,\n",
    "                inference_cfg.class_score_thresholds,\n",
    "                inference_cfg.draw_ground_truth,\n",
    "                output_path,\n",
    "            )\n",
    "\n",
    "    metrics = compute_detection_metrics(\n",
    "        predictions, targets_for_eval, dataset_cfg.num_classes, train_cfg.iou_threshold\n",
    "    )\n",
    "    metrics[\"false_positive_images\"] = fp_records\n",
    "    metrics[\"false_positive_stems\"] = sorted({record[\"image_id\"] for record in fp_records})\n",
    "    metric_lines = format_epoch_metrics(\n",
    "        epoch=None,\n",
    "        train_loss=None,\n",
    "        metrics=metrics,\n",
    "        dataset_cfg=dataset_cfg,\n",
    "        header=f\"Inference @ IoU {train_cfg.iou_threshold:.2f}\",\n",
    "    )\n",
    "    emit_metric_lines(metric_lines, logger=LOGGER)\n",
    "\n",
    "    if fp_report_path is not None:\n",
    "        fp_report_path = Path(fp_report_path)\n",
    "        write_false_positive_report(\n",
    "            fp_records,\n",
    "            fp_report_path,\n",
    "            split=split or dataset_cfg.test_split,\n",
    "            score_threshold=inference_cfg.score_threshold,\n",
    "            class_score_thresholds=inference_cfg.class_score_thresholds,\n",
    "            iou_threshold=train_cfg.iou_threshold,\n",
    "        )\n",
    "        LOGGER.info(\n",
    "            \"Wrote false-positive report for %d images to %s\",\n",
    "            len(fp_records),\n",
    "            fp_report_path,\n",
    "        )\n",
    "\n",
    "    if fp_list_path is not None:\n",
    "        fp_list_path = Path(fp_list_path)\n",
    "        write_false_positive_list(fp_records, fp_list_path)\n",
    "        LOGGER.info(\n",
    "            \"Wrote %d image ids with false positives to %s\",\n",
    "            len({str(record[\"image_id\"]) for record in fp_records}),\n",
    "            fp_list_path,\n",
    "        )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cfg = DatasetConfig(base_dir=Path('/kaggle/input/electrical-component/dataset'))\n",
    "train_cfg = TrainingConfig(epochs=7, batch_size=2, augmentation=False)\n",
    "inference_cfg = InferenceConfig(score_threshold=0.6, draw_ground_truth=True)\n",
    "\n",
    "model, history = train_pipeline(dataset_cfg, train_cfg)\n",
    "metrics = run_inference(\n",
    "    dataset_cfg,\n",
    "    inference_cfg,\n",
    "    train_cfg,\n",
    "    checkpoint_path=train_cfg.checkpoint_path,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提示：** `run_inference` 会在返回的 `metrics` 中附带 `false_positive_images` 和 `false_positive_stems`。 如果想在下一次训练时忽略这些样本，可以将 `TrainingConfig.exclude_samples` 设为 `tuple(metrics[\"false_positive_stems\"])`，或将 `metrics[\"false_positive_stems\"]` 写入文本文件， 然后通过命令行参数 `--exclude-list` 传入。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
