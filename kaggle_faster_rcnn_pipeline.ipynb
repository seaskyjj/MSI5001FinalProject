{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13448831,"sourceType":"datasetVersion","datasetId":8456082},{"sourceId":622715,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":468438,"modelId":484289}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":7814.129974,"end_time":"2025-10-21T09:11:26.346868","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-21T07:01:12.216894","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"254ff11c","cell_type":"markdown","source":"# Electrical Component Detection Pipeline\n\nThis notebook consolidates the refactored Faster R-CNN training and inference workflow into a single place for convenient experimentation on Kaggle. It provides reusable configuration objects, dataset loaders with optional augmentation, detailed metric utilities (including per-class TP/FP/FN and mAP), and helpers for both training and inference.\n","metadata":{"papermill":{"duration":0.004731,"end_time":"2025-10-21T07:01:15.851900","exception":false,"start_time":"2025-10-21T07:01:15.847169","status":"completed"},"tags":[]}},{"id":"35ea6263","cell_type":"code","source":"from __future__ import annotations\n\nimport argparse\nimport contextlib\nimport inspect\nimport json\nimport logging\nimport math\nimport multiprocessing as mp\nimport os\nimport random\nimport warnings\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Dict, Iterable, List, Mapping, Optional, Sequence, Set, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image as PILImage, ImageDraw, ImageEnhance, ImageFont\nfrom torch import Tensor, nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models.detection import (\n    FasterRCNN_ResNet50_FPN_V2_Weights,\n    fasterrcnn_resnet50_fpn_v2,\n)\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.rpn import AnchorGenerator, RPNHead\nfrom torchvision.transforms import InterpolationMode\nfrom torchvision.transforms import functional as TVF\nfrom tqdm import tqdm\n","metadata":{"execution":{"iopub.status.busy":"2025-10-29T06:05:02.098597Z","iopub.execute_input":"2025-10-29T06:05:02.098846Z","iopub.status.idle":"2025-10-29T06:05:08.983942Z","shell.execute_reply.started":"2025-10-29T06:05:02.098826Z","shell.execute_reply":"2025-10-29T06:05:08.983312Z"},"papermill":{"duration":11.415509,"end_time":"2025-10-21T07:01:27.271402","exception":false,"start_time":"2025-10-21T07:01:15.855893","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":2},{"id":"85042471","cell_type":"markdown","source":"## Configuration objects\n","metadata":{"papermill":{"duration":0.003251,"end_time":"2025-10-21T07:01:27.278475","exception":false,"start_time":"2025-10-21T07:01:27.275224","status":"completed"},"tags":[]}},{"id":"a1e69414","cell_type":"code","source":"\"\"\"Configuration objects for the electrical component detection project.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\n\n\nDEFAULT_PRETRAINED_URL = (\n    \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\"\n)\n\n#0.999 menas class does not exist\nDEFAULT_CLASS_SCORE_THRESHOLDS = {\n    3: 0.999,  \n    6: 0.8,\n    7: 0.9,\n    8: 0.999,\n    12: 0.999,\n    16: 0.97,\n    17: 0.999,\n    20: 0.9,\n    21: 0.9,\n    24: 0.999,\n    25: 0.97,\n    26: 0.999,\n    30: 0.95,\n}\n\n\n@dataclass\nclass DatasetConfig:\n    \"\"\"Configuration describing the dataset layout and metadata.\"\"\"\n\n    base_dir: Path = Path(\"data\")\n    train_split: str = \"train\"\n    valid_split: str = \"valid\"\n    test_split: str = \"test\"\n    image_folder: str = \"images\"\n    label_folder: str = \"labels\"\n    num_classes: int = 32\n    class_names: Tuple[str, ...] = ()\n\n    def __post_init__(self) -> None:\n        if not self.class_names:\n            # Fallback names are useful for logging when a mapping file is not provided.\n            self.class_names = tuple(f\"class_{idx:02d}\" for idx in range(self.num_classes))\n\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Hyper-parameters and runtime settings for model training.\"\"\"\n\n    epochs: int = 20\n    batch_size: int = 4\n    learning_rate: float = 5e-5\n    weight_decay: float = 5e-5\n    num_workers: int = 0\n    amp: bool = True\n    augmentation: bool = True\n    mosaic_prob: float = 0.6\n    mixup_prob: float = 0.6\n    mixup_alpha: float = 0.4\n    scale_jitter_min: float = 0.8\n    scale_jitter_max: float = 1.2\n    rotation_prob: float = 0.5\n    rotation_max_degrees: float = 30.0\n    affine_prob: float = 0.3\n    affine_translate: Tuple[float, float] = (0.1, 0.1)\n    affine_scale_range: Tuple[float, float] = (0.9, 1.1)\n    affine_shear: Tuple[float, float] = (5, 5)\n    small_object: bool = True\n    score_threshold: float = 0.6\n    iou_threshold: float = 0.5\n    eval_interval: int = 1\n    seed: int = 37\n    output_dir: Path = Path(\"outputs\")\n    checkpoint_path: Path = Path(\"outputs/best_model.pth\")\n    pretrained_weights_path: Path = Path(\"weights/fasterrcnn_resnet50_fpn_v2_coco.pth\")\n    pretrained_weights_url: str = DEFAULT_PRETRAINED_URL\n    log_every: int = 20\n    resume: bool = False\n    resume_path: Optional[Path] = None\n    last_checkpoint_path: Path = Path(\"outputs/last_checkpoint.pth\")\n    class_score_thresholds: Dict[int, float] = field(\n        default_factory=lambda: DEFAULT_CLASS_SCORE_THRESHOLDS.copy()\n    )\n    exclude_samples: Tuple[str, ...] = tuple()\n    fp_visual_dir: Optional[Path] = Path(\"outputs/fp_images\")\n    fp_report_path: Optional[Path] = None\n    fp_list_path: Optional[Path] = None\n    fp_classes: Tuple[int, ...] = (16, 30)\n\n    def ensure_directories(self) -> None:\n        \"\"\"Create output directories if they do not exist.\"\"\"\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.pretrained_weights_path.parent.mkdir(parents=True, exist_ok=True)\n        self.checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n        self.last_checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n        if self.fp_visual_dir:\n            self.fp_visual_dir.mkdir(parents=True, exist_ok=True)\n\n\n@dataclass\nclass InferenceConfig:\n    \"\"\"Options for running model inference and visualisation.\"\"\"\n\n    score_threshold: float = 0.7\n    max_images: int = 200\n    output_dir: Path = Path(\"outputs/inference\")\n    draw_ground_truth: bool = True\n    class_colors: List[str] = field(default_factory=list)\n    class_score_thresholds: Dict[int, float] = field(\n        default_factory=lambda: DEFAULT_CLASS_SCORE_THRESHOLDS.copy()\n    )\n\n    def ensure_directories(self) -> None:\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n","metadata":{"execution":{"iopub.status.busy":"2025-10-29T06:05:08.985251Z","iopub.execute_input":"2025-10-29T06:05:08.985646Z","iopub.status.idle":"2025-10-29T06:05:09.001720Z","shell.execute_reply.started":"2025-10-29T06:05:08.985619Z","shell.execute_reply":"2025-10-29T06:05:09.000967Z"},"papermill":{"duration":0.015709,"end_time":"2025-10-21T07:01:27.297445","exception":false,"start_time":"2025-10-21T07:01:27.281736","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":3},{"id":"182b06fe","cell_type":"markdown","source":"## Dataset loading and augmentation\n","metadata":{"papermill":{"duration":0.002924,"end_time":"2025-10-21T07:01:27.303648","exception":false,"start_time":"2025-10-21T07:01:27.300724","status":"completed"},"tags":[]}},{"id":"4f9c00ab","cell_type":"code","source":"\"\"\"Dataset and data loading utilities for electrical component detection.\"\"\"\n\nimport logging\nimport math\nimport multiprocessing as mp\nimport os\nimport random\nimport warnings\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, Iterable, List, Optional, Set, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image as PILImage, ImageEnhance\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.transforms import InterpolationMode\nfrom torchvision.transforms import functional as TVF\n\n\n\n\nLOGGER = logging.getLogger(__name__)\n\n\n@dataclass\nclass AugmentationParams:\n    \"\"\"Parameters controlling the dataset level image augmentations.\"\"\"\n\n    horizontal_flip_prob: float = 0.5\n    vertical_flip_prob: float = 0.2\n    brightness: float = 0.2\n    contrast: float = 0.2\n    saturation: float = 0.2\n    hue: float = 0.02\n    rotation_prob: float = 0.0\n    rotation_max_degrees: float = 0.0\n    affine_prob: float = 0.0\n    affine_translate: Tuple[float, float] = (0.0, 0.0)\n    affine_scale_range: Tuple[float, float] = (1.0, 1.0)\n    affine_shear: Tuple[float, float] = (0.0, 0.0)\n    mosaic_prob: float = 0.0\n    mixup_prob: float = 0.0\n    mixup_alpha: float = 0.4\n    scale_jitter_range: Tuple[float, float] = (1.0, 1.0)\n\n\ndef load_image_hwc_uint8(path: Path) -> np.ndarray:\n    \"\"\"Load an ``.npy`` image stored as HWC and return an ``uint8`` array.\"\"\"\n    image = np.load(path, allow_pickle=False, mmap_mode=\"r\")\n\n    if image.dtype != np.uint8:\n        image = image.astype(np.float32, copy=False)\n        vmin, vmax = float(image.min()), float(image.max())\n        if 0.0 <= vmin and vmax <= 1.0:\n            image = (image * 255.0).round()\n        elif -1.0 <= vmin and vmax <= 1.0:\n            image = ((image + 1.0) * 0.5 * 255.0).round()\n        image = np.clip(image, 0, 255).astype(np.uint8)\n\n    channels = image.shape[2]\n    if channels == 1:\n        image = np.repeat(image, 3, axis=2)\n    elif channels == 4:\n        image = image[..., :3]\n    if not image.flags.writeable or not image.flags.c_contiguous:\n        image = np.array(image, copy=True)\n    return image\n\n\nclass ElectricalComponentsDataset(Dataset):\n    \"\"\"Dataset of electrical component detections stored as ``.npy`` images and CSV labels.\"\"\"\n\n    def __init__(\n        self,\n        root: Path,\n        split: str,\n        class_names: Iterable[str],\n        transform: Optional[AugmentationParams] = None,\n        use_augmentation: bool = False,\n        exclude_stems: Optional[Iterable[str]] = None,\n    ) -> None:\n        self.root = Path(root)\n        self.split = split\n        self.class_names = list(class_names)\n        self.transform_params = transform or AugmentationParams()\n        self.use_augmentation = use_augmentation\n\n        self.image_dir = self.root / split / \"images\"\n        self.label_dir = self.root / split / \"labels\"\n\n        if not self.image_dir.exists():\n            raise FileNotFoundError(f\"Missing image directory: {self.image_dir}\")\n        if not self.label_dir.exists():\n            raise FileNotFoundError(f\"Missing label directory: {self.label_dir}\")\n\n        self.image_stems = sorted(p.stem for p in self.label_dir.glob(\"*.csv\"))\n        if not self.image_stems:\n            raise RuntimeError(f\"No label files found in {self.label_dir}\")\n\n        exclude_set: Set[str] = set()\n        if exclude_stems:\n            exclude_set = {Path(stem).stem for stem in exclude_stems}\n            if exclude_set:\n                before = len(self.image_stems)\n                self.image_stems = [stem for stem in self.image_stems if stem not in exclude_set]\n                removed = before - len(self.image_stems)\n                if removed:\n                    LOGGER.info(\n                        \"Split %s: excluded %d samples based on provided stem filter.\",\n                        self.split,\n                        removed,\n                    )\n\n        self.excluded_stems = sorted(exclude_set)\n\n        # Pre-load all annotations to reduce I/O during training.\n        self.annotations: Dict[str, pd.DataFrame] = {\n            stem: pd.read_csv(self.label_dir / f\"{stem}.csv\") for stem in self.image_stems\n        }\n\n    def __len__(self) -> int:\n        return len(self.image_stems)\n\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        stem = self.image_stems[index]\n        image, boxes, labels = self._load_raw_sample(stem)\n\n        if self.use_augmentation:\n            image, boxes, labels = self._apply_composite_augmentations(stem, image, boxes, labels)\n            image, boxes = self._apply_augmentations(image, boxes)\n\n        height, width = image.shape[:2]\n\n        image_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n        boxes_tensor = torch.from_numpy(boxes).float() if boxes.size else torch.zeros((0, 4), dtype=torch.float32)\n        labels_tensor = (\n            torch.from_numpy(labels).long() if labels.size else torch.zeros((0,), dtype=torch.long)\n        )\n\n        boxes_tensor, labels_tensor = sanitize_boxes_and_labels(\n            boxes_tensor, labels_tensor, height, width\n        )\n\n        target: Dict[str, torch.Tensor] = {\n            \"boxes\": boxes_tensor,\n            \"labels\": labels_tensor,\n            \"image_id\": torch.tensor(index, dtype=torch.int64),\n            \"area\": (boxes_tensor[:, 2] - boxes_tensor[:, 0])\n            * (boxes_tensor[:, 3] - boxes_tensor[:, 1])\n            if boxes_tensor.numel()\n            else torch.tensor([], dtype=torch.float32),\n            \"iscrowd\": torch.zeros((boxes_tensor.shape[0],), dtype=torch.int64),\n            \"orig_size\": torch.tensor([height, width], dtype=torch.int64),\n        }\n\n        return image_tensor, target\n\n    def _load_raw_sample(self, stem: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        image_path = self.image_dir / f\"{stem}.npy\"\n        image = load_image_hwc_uint8(image_path)\n        height, width = image.shape[:2]\n\n        ann = self.annotations[stem]\n        boxes, labels = self._annotation_to_boxes(ann, width, height)\n        return image, boxes, labels\n\n    @staticmethod\n    def _annotation_to_boxes(\n        ann: pd.DataFrame, width: int, height: int\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        if ann.empty:\n            return np.zeros((0, 4), dtype=np.float32), np.zeros((0,), dtype=np.int64)\n\n        x_center = ann[\"x_center\"].to_numpy(dtype=np.float32)\n        y_center = ann[\"y_center\"].to_numpy(dtype=np.float32)\n        box_width = ann[\"width\"].to_numpy(dtype=np.float32)\n        box_height = ann[\"height\"].to_numpy(dtype=np.float32)\n\n        # Auto-detect normalised coordinates and scale back to pixel space.\n        if (\n            (x_center.size == 0 or float(x_center.max()) <= 1.0)\n            and (y_center.size == 0 or float(y_center.max()) <= 1.0)\n            and (box_width.size == 0 or float(box_width.max()) <= 1.0)\n            and (box_height.size == 0 or float(box_height.max()) <= 1.0)\n        ):\n            x_center = x_center * width\n            y_center = y_center * height\n            box_width = box_width * width\n            box_height = box_height * height\n\n        x1 = x_center - box_width / 2.0\n        y1 = y_center - box_height / 2.0\n        x2 = x_center + box_width / 2.0\n        y2 = y_center + box_height / 2.0\n\n        boxes = np.stack([x1, y1, x2, y2], axis=1).astype(np.float32)\n        labels = ann[\"class\"].to_numpy(dtype=np.int64) + 1  # shift to 1..K so 0 remains reserved for background\n        return boxes, labels\n\n    def _apply_composite_augmentations(\n        self,\n        stem: str,\n        image: np.ndarray,\n        boxes: np.ndarray,\n        labels: np.ndarray,\n    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        params = self.transform_params\n\n        if (\n            params.mosaic_prob > 0.0\n            and random.random() < params.mosaic_prob\n            and len(self.image_stems) >= 4\n        ):\n            image, boxes, labels = self._apply_mosaic(stem, image, boxes, labels)\n\n        if (\n            params.mixup_prob > 0.0\n            and params.mixup_alpha > 0.0\n            and random.random() < params.mixup_prob\n        ):\n            image, boxes, labels = self._apply_mixup(stem, image, boxes, labels)\n\n        if params.scale_jitter_range != (1.0, 1.0):\n            image, boxes = self._apply_scale_jitter(image, boxes, params.scale_jitter_range)\n\n        return image, boxes, labels\n\n    def _apply_augmentations(\n        self, image: np.ndarray, boxes: np.ndarray\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        params = self.transform_params\n        height, width = image.shape[:2]\n\n        if (\n            params.rotation_prob > 0.0\n            and params.rotation_max_degrees > 0.0\n            and random.random() < params.rotation_prob\n        ):\n            angle = random.uniform(-params.rotation_max_degrees, params.rotation_max_degrees)\n            image, boxes = self._apply_affine_transform(\n                image,\n                boxes,\n                angle=angle,\n                translate=(0.0, 0.0),\n                scale=1.0,\n                shear=(0.0, 0.0),\n            )\n            height, width = image.shape[:2]\n\n        if params.affine_prob > 0.0 and random.random() < params.affine_prob:\n            max_tx = abs(params.affine_translate[0]) * width\n            max_ty = abs(params.affine_translate[1]) * height\n            translate = (\n                random.uniform(-max_tx, max_tx),\n                random.uniform(-max_ty, max_ty),\n            )\n\n            scale_min, scale_max = params.affine_scale_range\n            if scale_min > scale_max:\n                scale_min, scale_max = scale_max, scale_min\n            scale_min = max(scale_min, 0.0)\n            scale_max = max(scale_max, 0.0)\n            scale = 1.0\n            if scale_max > 0.0:\n                if math.isclose(scale_min, scale_max):\n                    scale = max(scale_min, 1e-3)\n                else:\n                    scale = max(random.uniform(scale_min, scale_max), 1e-3)\n\n            shear_x = params.affine_shear[0]\n            shear_y = params.affine_shear[1]\n            shear = (\n                random.uniform(-abs(shear_x), abs(shear_x)),\n                random.uniform(-abs(shear_y), abs(shear_y)),\n            )\n\n            image, boxes = self._apply_affine_transform(\n                image,\n                boxes,\n                angle=0.0,\n                translate=translate,\n                scale=scale,\n                shear=shear,\n            )\n            height, width = image.shape[:2]\n\n        if boxes.size and random.random() < params.horizontal_flip_prob:\n            image = np.ascontiguousarray(image[:, ::-1, :])\n            x1 = width - boxes[:, 2]\n            x2 = width - boxes[:, 0]\n            boxes[:, 0], boxes[:, 2] = x1, x2\n\n        if boxes.size and random.random() < params.vertical_flip_prob:\n            image = np.ascontiguousarray(image[::-1, :, :])\n            y1 = height - boxes[:, 3]\n            y2 = height - boxes[:, 1]\n            boxes[:, 1], boxes[:, 3] = y1, y2\n\n        if params.brightness or params.contrast or params.saturation or params.hue:\n            pil = PILImage.fromarray(image)\n            if params.brightness:\n                enhancer = ImageEnhance.Brightness(pil)\n                factor = 1.0 + random.uniform(-params.brightness, params.brightness)\n                pil = enhancer.enhance(max(0.1, factor))\n            if params.contrast:\n                enhancer = ImageEnhance.Contrast(pil)\n                factor = 1.0 + random.uniform(-params.contrast, params.contrast)\n                pil = enhancer.enhance(max(0.1, factor))\n            if params.saturation:\n                enhancer = ImageEnhance.Color(pil)\n                factor = 1.0 + random.uniform(-params.saturation, params.saturation)\n                pil = enhancer.enhance(max(0.1, factor))\n            if params.hue:\n                hsv_image = pil.convert(\"HSV\")\n                h_channel, s_channel, v_channel = hsv_image.split()\n                delta = int(params.hue * 255.0 * random.choice([-1, 1]))\n                h_channel = h_channel.point(lambda h: (h + delta) % 255)\n                hsv_image = PILImage.merge(\"HSV\", (h_channel, s_channel, v_channel))\n                pil = hsv_image.convert(\"RGB\")\n            image = np.array(pil)\n\n        if boxes.size:\n            boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, width)\n            boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, height)\n        return image, boxes\n\n    def _apply_scale_jitter(\n        self, image: np.ndarray, boxes: np.ndarray, scale_range: Tuple[float, float]\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        min_scale, max_scale = scale_range\n        if max_scale <= 0 or min_scale <= 0:\n            return image, boxes\n\n        factor = random.uniform(min_scale, max_scale)\n        if np.isclose(factor, 1.0):\n            return image, boxes\n\n        height, width = image.shape[:2]\n        new_height = max(1, int(round(height * factor)))\n        new_width = max(1, int(round(width * factor)))\n\n        pil = PILImage.fromarray(image)\n        resized = pil.resize((new_width, new_height), resample=PILImage.BILINEAR)\n        image = np.array(resized)\n\n        if boxes.size:\n            boxes = boxes.copy()\n            boxes[:, [0, 2]] *= float(new_width) / float(width)\n            boxes[:, [1, 3]] *= float(new_height) / float(height)\n        return image, boxes\n\n    def _apply_affine_transform(\n        self,\n        image: np.ndarray,\n        boxes: np.ndarray,\n        *,\n        angle: float,\n        translate: Tuple[float, float],\n        scale: float,\n        shear: Tuple[float, float],\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        if not np.isfinite(scale) or scale <= 0.0:\n            scale = 1.0\n\n        height, width = image.shape[:2]\n        pil = PILImage.fromarray(image)\n        translate_int = (\n            int(round(float(translate[0]))),\n            int(round(float(translate[1]))),\n        )\n        transformed = TVF.affine(\n            pil,\n            angle=float(angle),\n            translate=translate_int,\n            scale=float(max(scale, 1e-3)),\n            shear=(float(shear[0]), float(shear[1])),\n            interpolation=InterpolationMode.BILINEAR,\n            fill=0,\n        )\n        image_out = np.array(transformed)\n        out_height, out_width = image_out.shape[:2]\n\n        if not boxes.size:\n            return image_out, boxes.astype(np.float32, copy=False)\n\n        matrix = _compute_affine_forward_matrix(\n            center=(width * 0.5, height * 0.5),\n            angle=float(angle),\n            translate=(float(translate_int[0]), float(translate_int[1])),\n            scale=float(max(scale, 1e-3)),\n            shear=(float(shear[0]), float(shear[1])),\n        )\n\n        corners = _boxes_to_corners(boxes)\n        ones = np.ones((corners.shape[0], 1), dtype=np.float32)\n        coords = np.concatenate([corners, ones], axis=1)\n        full_matrix = np.vstack([matrix, [0.0, 0.0, 1.0]])\n        transformed_coords = coords @ full_matrix.T\n        transformed_corners = transformed_coords[:, :2].reshape(-1, 4, 2)\n        min_xy = transformed_corners.min(axis=1)\n        max_xy = transformed_corners.max(axis=1)\n\n        boxes_out = np.concatenate([min_xy, max_xy], axis=1)\n        boxes_out[:, [0, 2]] = boxes_out[:, [0, 2]].clip(0, out_width)\n        boxes_out[:, [1, 3]] = boxes_out[:, [1, 3]].clip(0, out_height)\n        return image_out, boxes_out.astype(np.float32, copy=False)\n\n    def _apply_mosaic(\n        self,\n        stem: str,\n        image: np.ndarray,\n        boxes: np.ndarray,\n        labels: np.ndarray,\n    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        height, width = image.shape[:2]\n        candidate_stems = [s for s in self.image_stems if s != stem]\n        if len(candidate_stems) < 3:\n            return image, boxes, labels\n\n        selected = random.sample(candidate_stems, 3)\n\n        images: List[np.ndarray] = [image]\n        boxes_list: List[np.ndarray] = [boxes]\n        labels_list: List[np.ndarray] = [labels]\n\n        for other_stem in selected:\n            other_img, other_boxes, other_labels = self._load_raw_sample(other_stem)\n            other_img, other_boxes = self._resize_like(other_img, other_boxes, width, height)\n            images.append(other_img)\n            boxes_list.append(other_boxes)\n            labels_list.append(other_labels)\n\n        canvas = np.zeros((height * 2, width * 2, 3), dtype=np.uint8)\n        offsets = [(0, 0), (0, width), (height, 0), (height, width)]\n        combined_boxes: List[np.ndarray] = []\n        combined_labels: List[np.ndarray] = []\n\n        for img, bxs, lbls, (y_off, x_off) in zip(images, boxes_list, labels_list, offsets):\n            canvas[y_off : y_off + height, x_off : x_off + width] = img\n            if bxs.size:\n                shifted = bxs.copy()\n                shifted[:, [0, 2]] += x_off\n                shifted[:, [1, 3]] += y_off\n                combined_boxes.append(shifted)\n                combined_labels.append(lbls)\n\n        if combined_boxes:\n            boxes = np.concatenate(combined_boxes, axis=0)\n            labels = np.concatenate(combined_labels, axis=0)\n        else:\n            boxes = np.zeros((0, 4), dtype=np.float32)\n            labels = np.zeros((0,), dtype=np.int64)\n\n        crop_x = random.randint(0, width)\n        crop_y = random.randint(0, height)\n        canvas = canvas[crop_y : crop_y + height, crop_x : crop_x + width]\n\n        if boxes.size:\n            boxes = boxes.copy()\n            boxes[:, [0, 2]] -= crop_x\n            boxes[:, [1, 3]] -= crop_y\n\n            keep = (\n                (boxes[:, 2] > 0)\n                & (boxes[:, 3] > 0)\n                & (boxes[:, 0] < width)\n                & (boxes[:, 1] < height)\n            )\n            boxes = boxes[keep]\n            labels = labels[keep]\n            boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, width)\n            boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, height)\n\n        return canvas, boxes, labels\n\n    def _apply_mixup(\n        self,\n        stem: str,\n        image: np.ndarray,\n        boxes: np.ndarray,\n        labels: np.ndarray,\n    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        other_stem = self._sample_alternative_stem(stem)\n        if other_stem is None:\n            return image, boxes, labels\n\n        other_img, other_boxes, other_labels = self._load_raw_sample(other_stem)\n        height, width = image.shape[:2]\n        other_img, other_boxes = self._resize_like(other_img, other_boxes, width, height)\n\n        alpha = max(self.transform_params.mixup_alpha, 1e-3)\n        lam = np.random.beta(alpha, alpha)\n        lam = float(np.clip(lam, 0.3, 0.7))\n\n        mixed = (\n            image.astype(np.float32) * lam + other_img.astype(np.float32) * (1.0 - lam)\n        ).astype(np.uint8)\n\n        if boxes.size and other_boxes.size:\n            boxes = np.concatenate([boxes, other_boxes], axis=0)\n            labels = np.concatenate([labels, other_labels], axis=0)\n        elif other_boxes.size:\n            boxes = other_boxes.copy()\n            labels = other_labels.copy()\n\n        return mixed, boxes, labels\n\n    def _sample_alternative_stem(self, current: str) -> Optional[str]:\n        if len(self.image_stems) <= 1:\n            return None\n        candidates = [stem for stem in self.image_stems if stem != current]\n        if not candidates:\n            return None\n        return random.choice(candidates)\n\n    @staticmethod\n    def _resize_like(\n        image: np.ndarray, boxes: np.ndarray, target_width: int, target_height: int\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        height, width = image.shape[:2]\n        if height == target_height and width == target_width:\n            return image, boxes\n\n        pil = PILImage.fromarray(image)\n        resized = pil.resize((target_width, target_height), resample=PILImage.BILINEAR)\n        image = np.array(resized)\n\n        if boxes.size:\n            boxes = boxes.copy()\n            boxes[:, [0, 2]] *= float(target_width) / float(width)\n            boxes[:, [1, 3]] *= float(target_height) / float(height)\n        return image, boxes\n\n\ndef _boxes_to_corners(boxes: np.ndarray) -> np.ndarray:\n    corners = np.stack(\n        [\n            boxes[:, [0, 1]],\n            boxes[:, [2, 1]],\n            boxes[:, [2, 3]],\n            boxes[:, [0, 3]],\n        ],\n        axis=1,\n    )\n    return corners.reshape(-1, 2).astype(np.float32, copy=False)\n\n\ndef _compute_affine_forward_matrix(\n    *,\n    center: Tuple[float, float],\n    angle: float,\n    translate: Tuple[float, float],\n    scale: float,\n    shear: Tuple[float, float],\n) -> np.ndarray:\n    rot = math.radians(angle)\n    shear_x = math.radians(shear[0])\n    shear_y = math.radians(shear[1])\n\n    cx, cy = center\n    tx, ty = translate\n\n    cos_sy = math.cos(shear_y)\n    if abs(cos_sy) < 1e-6:\n        cos_sy = 1e-6 if cos_sy >= 0 else -1e-6\n\n    a = math.cos(rot - shear_y) / cos_sy\n    b = -math.cos(rot - shear_y) * math.tan(shear_x) / cos_sy - math.sin(rot)\n    c = math.sin(rot - shear_y) / cos_sy\n    d = -math.sin(rot - shear_y) * math.tan(shear_x) / cos_sy + math.cos(rot)\n\n    matrix = [a, b, 0.0, c, d, 0.0]\n    matrix = [x * scale for x in matrix]\n\n    matrix[2] += matrix[0] * (-cx) + matrix[1] * (-cy)\n    matrix[5] += matrix[3] * (-cx) + matrix[4] * (-cy)\n    matrix[2] += cx + tx\n    matrix[5] += cy + ty\n\n    return np.array(matrix, dtype=np.float32).reshape(2, 3)\n\n\ndef detection_collate(batch: List[Tuple[torch.Tensor, Dict[str, torch.Tensor]]]):\n    \"\"\"Collate function for detection datasets returning lists of tensors.\"\"\"\n    images, targets = zip(*batch)\n    return list(images), list(targets)\n\n\ndef _safe_worker_count(requested: int) -> int:\n    cpu_count = os.cpu_count() or 1\n    if requested <= 0:\n        return 0\n    # Leave one core free so that the main process remains responsive on small machines.\n    max_workers = max(1, cpu_count - 1)\n    return min(requested, max_workers)\n\n\ndef _should_force_single_worker(dataset: Dataset) -> bool:\n    \"\"\"Determine whether multiprocessing workers should be disabled.\"\"\"\n\n    module_name = getattr(dataset.__class__, \"__module__\", \"\")\n    if module_name in {\"__main__\", \"__mp_main__\", \"builtins\"}:\n        return True\n\n    if module_name.startswith(\"ipykernel\"):  # pragma: no cover - notebook specific\n        return True\n\n    return running_in_ipython_kernel()\n\n\ndef create_data_loaders(\n    dataset: Dataset,\n    batch_size: int,\n    shuffle: bool,\n    num_workers: int,\n) -> DataLoader:\n    \"\"\"Create a :class:`~torch.utils.data.DataLoader` for the detection dataset.\n\n    Kaggle notebooks occasionally run in restricted multiprocessing environments where\n    ``fork`` based workers cannot be reaped cleanly.  We therefore favour a conservative\n    default (``num_workers=0``), detect in-notebook dataset definitions that cannot be\n    spawned safely, and fall back to single-process loading automatically when worker\n    start-up fails.\n    \"\"\"\n\n    worker_count = _safe_worker_count(num_workers)\n    if worker_count > 0 and _should_force_single_worker(dataset):\n        LOGGER.info(\n            \"Detected interactive environment or in-notebook dataset definition; forcing num_workers=0.\"\n        )\n        worker_count = 0\n\n    loader_kwargs = dict(\n        dataset=dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        pin_memory=torch.cuda.is_available(),\n        collate_fn=detection_collate,\n    )\n\n    if worker_count > 0:\n        loader_kwargs[\"num_workers\"] = worker_count\n        loader_kwargs[\"persistent_workers\"] = True\n        # ``spawn`` avoids PID mismatches that surface as ``AssertionError: can only\n        # test a child process`` when the notebook kernel re-uses processes.\n        loader_kwargs[\"multiprocessing_context\"] = mp.get_context(\"spawn\")\n    else:\n        loader_kwargs[\"num_workers\"] = 0\n\n    try:\n        return DataLoader(**loader_kwargs)\n    except (RuntimeError, OSError, AssertionError) as exc:\n        if worker_count == 0:\n            raise\n        warnings.warn(\n            \"Falling back to num_workers=0 because DataLoader worker initialisation \"\n            f\"failed with: {exc}\",\n            RuntimeWarning,\n        )\n        LOGGER.warning(\"DataLoader workers failed to start (%s). Using num_workers=0 instead.\", exc)\n        loader_kwargs.pop(\"persistent_workers\", None)\n        loader_kwargs.pop(\"multiprocessing_context\", None)\n        loader_kwargs[\"num_workers\"] = 0\n        return DataLoader(**loader_kwargs)\n","metadata":{"execution":{"iopub.status.busy":"2025-10-29T06:05:09.002886Z","iopub.execute_input":"2025-10-29T06:05:09.003257Z","iopub.status.idle":"2025-10-29T06:05:09.074877Z","shell.execute_reply.started":"2025-10-29T06:05:09.003232Z","shell.execute_reply":"2025-10-29T06:05:09.074071Z"},"papermill":{"duration":0.032579,"end_time":"2025-10-21T07:01:27.339401","exception":false,"start_time":"2025-10-21T07:01:27.306822","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"id":"bd0e689c","cell_type":"markdown","source":"## Utility helpers and metrics\n","metadata":{"papermill":{"duration":0.002974,"end_time":"2025-10-21T07:01:27.345614","exception":false,"start_time":"2025-10-21T07:01:27.342640","status":"completed"},"tags":[]}},{"id":"3637ad51","cell_type":"code","source":"\"\"\"Utility helpers for training and evaluating the detection model.\"\"\"\n\nimport json\nimport logging\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Iterable, List, Mapping, Optional, Sequence, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom PIL import Image as PILImage, ImageDraw, ImageFont\nfrom torch import Tensor\n\n\n\n\n# ``DEFAULT_COLORS`` previously lived in the Kaggle notebook version of the\n# project.  The constant was referenced when rendering detections but never\n# actually defined in the standalone module, which caused a ``NameError`` when\n# the training script attempted to export visualisations.  Keeping the palette\n# here restores the expected behaviour while remaining independent from the\n# notebook.\nDEFAULT_COLORS: Tuple[str, ...] = (\n    \"#FF6B6B\",\n    \"#4ECDC4\",\n    \"#FFD93D\",\n    \"#1A535C\",\n    \"#FF9F1C\",\n    \"#2EC4B6\",\n    \"#E71D36\",\n    \"#9B5DE5\",\n    \"#F15BB5\",\n    \"#00BBF9\",\n    \"#00F5D4\",\n    \"#6C5CE7\",\n    \"#45B7D1\",\n    \"#F9C80E\",\n    \"#F86624\",\n    \"#EA3546\",\n    \"#662E9B\",\n    \"#43BCCD\",\n    \"#A1C181\",\n    \"#BB9F06\",\n)\n\n\ndef set_seed(seed: int) -> None:\n    \"\"\"Set seeds for the Python, NumPy and PyTorch RNGs.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef sanitize_boxes_and_labels(\n    boxes: Tensor, labels: Tensor, height: int, width: int, min_size: float = 1.0\n) -> Tuple[Tensor, Tensor]:\n    \"\"\"Clamp bounding boxes to the image size and drop invalid boxes.\"\"\"\n    if boxes.numel() == 0:\n        return boxes.reshape(0, 4).float(), labels.reshape(0).long()\n\n    boxes = boxes.clone()\n    boxes[:, 0::2] = boxes[:, 0::2].clamp(0, float(width))\n    boxes[:, 1::2] = boxes[:, 1::2].clamp(0, float(height))\n\n    widths = boxes[:, 2] - boxes[:, 0]\n    heights = boxes[:, 3] - boxes[:, 1]\n    keep = (widths > min_size) & (heights > min_size)\n\n    if keep.sum() == 0:\n        return boxes.new_zeros((0, 4)), labels.new_zeros((0,), dtype=torch.long)\n    return boxes[keep].float(), labels[keep].long()\n\n\ndef compute_iou_matrix(boxes1: np.ndarray, boxes2: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the IoU matrix between two sets of boxes in ``xyxy`` format.\"\"\"\n    if boxes1.size == 0 or boxes2.size == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]), dtype=np.float32)\n\n    x11, y11, x12, y12 = np.split(boxes1, 4, axis=1)\n    x21, y21, x22, y22 = np.split(boxes2, 4, axis=1)\n\n    inter_x1 = np.maximum(x11, x21.T)\n    inter_y1 = np.maximum(y11, y21.T)\n    inter_x2 = np.minimum(x12, x22.T)\n    inter_y2 = np.minimum(y12, y22.T)\n\n    inter_w = np.clip(inter_x2 - inter_x1, a_min=0.0, a_max=None)\n    inter_h = np.clip(inter_y2 - inter_y1, a_min=0.0, a_max=None)\n    inter_area = inter_w * inter_h\n\n    area1 = (x12 - x11) * (y12 - y11)\n    area2 = (x22 - x21) * (y22 - y21)\n\n    union = area1 + area2.T - inter_area\n    return np.divide(inter_area, union, out=np.zeros_like(inter_area), where=union > 0)\n\n\ndef compute_average_precision(recalls: np.ndarray, precisions: np.ndarray) -> float:\n    \"\"\"Compute the interpolated Average Precision (AP) following COCO style.\"\"\"\n    if recalls.size == 0 or precisions.size == 0:\n        return 0.0\n\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n\n    recall_points = np.linspace(0, 1, 101)\n    precision_interp = np.interp(recall_points, mrec, mpre)\n    return float(np.trapz(precision_interp, recall_points))\n\n\ndef accumulate_classification_stats(\n    predictions: Sequence[Dict[str, np.ndarray]],\n    targets: Sequence[Dict[str, np.ndarray]],\n    num_classes: int,\n    iou_threshold: float,\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[List[float]], List[List[int]], np.ndarray]:\n    \"\"\"Accumulate TP/FP/FN statistics and matched predictions for AP computation.\"\"\"\n    tp = np.zeros(num_classes, dtype=np.int64)\n    fp = np.zeros(num_classes, dtype=np.int64)\n    fn = np.zeros(num_classes, dtype=np.int64)\n    scores: List[List[float]] = [[] for _ in range(num_classes)]\n    matches: List[List[int]] = [[] for _ in range(num_classes)]\n    gt_counter = np.zeros(num_classes, dtype=np.int64)\n\n    for pred, tgt in zip(predictions, targets):\n        pred_boxes = pred[\"boxes\"]\n        pred_scores = pred[\"scores\"]\n        pred_labels = pred[\"labels\"].astype(np.int64)\n\n        gt_boxes = tgt[\"boxes\"]\n        gt_labels = tgt[\"labels\"].astype(np.int64)\n\n        unique_classes = np.unique(np.concatenate((pred_labels, gt_labels)))\n        for cls in unique_classes:\n            pb = pred_boxes[pred_labels == cls]\n            ps = pred_scores[pred_labels == cls]\n            tb = gt_boxes[gt_labels == cls]\n            gt_counter[cls] += len(tb)\n\n            if len(tb) == 0:\n                fp[cls] += len(pb)\n                scores[cls].extend(ps.tolist())\n                matches[cls].extend([0] * len(pb))\n                continue\n\n            order = np.argsort(-ps)\n            pb = pb[order]\n            ps = ps[order]\n            iou_matrix = compute_iou_matrix(pb, tb)\n\n            matched_gt: set[int] = set()\n            for det_idx, score in enumerate(ps):\n                if tb.size == 0:\n                    fp[cls] += 1\n                    scores[cls].append(float(score))\n                    matches[cls].append(0)\n                    continue\n\n                best_gt = int(np.argmax(iou_matrix[det_idx]))\n                best_iou = iou_matrix[det_idx, best_gt]\n\n                if best_iou >= iou_threshold and best_gt not in matched_gt:\n                    tp[cls] += 1\n                    matched_gt.add(best_gt)\n                    scores[cls].append(float(score))\n                    matches[cls].append(1)\n                else:\n                    fp[cls] += 1\n                    scores[cls].append(float(score))\n                    matches[cls].append(0)\n\n            fn[cls] += len(tb) - len(matched_gt)\n\n    return tp, fp, fn, scores, matches, gt_counter\n\n\ndef identify_false_positive_predictions(\n    prediction: Dict[str, np.ndarray],\n    target: Dict[str, np.ndarray],\n    num_classes: int,\n    iou_threshold: float,\n) -> List[Dict[str, Union[int, float, List[float]]]]:\n    \"\"\"Return detailed records for false-positive detections in a single sample.\"\"\"\n\n    boxes = np.asarray(prediction.get(\"boxes\", np.empty((0, 4), dtype=np.float32)), dtype=np.float32)\n    scores = np.asarray(prediction.get(\"scores\", np.empty((0,), dtype=np.float32)), dtype=np.float32)\n    labels = np.asarray(prediction.get(\"labels\", np.empty((0,), dtype=np.int64)), dtype=np.int64)\n\n    gt_boxes = np.asarray(target.get(\"boxes\", np.empty((0, 4), dtype=np.float32)), dtype=np.float32)\n    gt_labels = np.asarray(target.get(\"labels\", np.empty((0,), dtype=np.int64)), dtype=np.int64)\n\n    if boxes.size == 0:\n        return []\n\n    fp_records: List[Dict[str, Union[int, float, List[float]]]] = []\n    unique_classes = np.unique(labels) if labels.size else np.asarray([], dtype=np.int64)\n\n    for cls in unique_classes:\n        cls = int(cls)\n        cls_mask = labels == cls\n        cls_indices = np.nonzero(cls_mask)[0]\n        if cls_indices.size == 0:\n            continue\n\n        pb = boxes[cls_mask]\n        ps = scores[cls_mask]\n        order = np.argsort(-ps)\n        pb_sorted = pb[order]\n        ps_sorted = ps[order]\n        original_indices = cls_indices[order]\n\n        tb = gt_boxes[gt_labels == cls]\n        iou_matrix = compute_iou_matrix(pb_sorted, tb) if tb.size else np.zeros((pb_sorted.shape[0], 0), dtype=np.float32)\n        matched_gt: set[int] = set()\n\n        for rank, (pred_idx, score_value) in enumerate(zip(original_indices, ps_sorted)):\n            if iou_matrix.shape[1]:\n                row = iou_matrix[rank]\n                best_gt = int(row.argmax())\n                best_iou = float(row[best_gt])\n            else:\n                best_gt = -1\n                best_iou = 0.0\n\n            is_true_positive = (\n                iou_matrix.shape[1] > 0\n                and best_iou >= iou_threshold\n                and best_gt not in matched_gt\n            )\n\n            if is_true_positive:\n                matched_gt.add(best_gt)\n                continue\n\n            fp_records.append(\n                {\n                    \"index\": int(pred_idx),\n                    \"class\": cls,\n                    \"score\": float(score_value),\n                    \"best_iou\": best_iou,\n                    \"box\": boxes[pred_idx].astype(float).tolist(),\n                }\n            )\n\n    return fp_records\n\n\ndef compute_detection_metrics(\n    predictions: Sequence[Dict[str, np.ndarray]],\n    targets: Sequence[Dict[str, np.ndarray]],\n    num_classes: int,\n    iou_threshold: float,\n) -> Dict[str, np.ndarray]:\n    \"\"\"Compute per-class metrics and mAP for detection results.\"\"\"\n    tp, fp, fn, scores, matches, gt_counter = accumulate_classification_stats(\n        predictions, targets, num_classes, iou_threshold\n    )\n\n    precision = np.divide(tp, np.clip(tp + fp, a_min=1, a_max=None))\n    recall = np.divide(tp, np.clip(tp + fn, a_min=1, a_max=None))\n\n    ap = np.zeros(num_classes, dtype=np.float32)\n    for cls in range(num_classes):\n        if gt_counter[cls] == 0:\n            ap[cls] = np.nan\n            continue\n        if not scores[cls]:\n            ap[cls] = 0.0\n            continue\n\n        order = np.argsort(-np.asarray(scores[cls]))\n        match_array = np.asarray(matches[cls], dtype=np.int32)[order]\n        cumulative_tp = np.cumsum(match_array)\n        cumulative_fp = np.cumsum(1 - match_array)\n\n        recalls = cumulative_tp / gt_counter[cls]\n        precisions = cumulative_tp / np.maximum(cumulative_tp + cumulative_fp, 1)\n        ap[cls] = compute_average_precision(recalls, precisions)\n\n    valid_ap = ap[np.isfinite(ap)]\n    map_value = float(valid_ap.mean()) if valid_ap.size else 0.0\n\n    return {\n        \"TP\": tp,\n        \"FP\": fp,\n        \"FN\": fn,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"AP\": ap,\n        \"mAP\": map_value,\n        \"gt_counter\": gt_counter,\n    }\n\n\ndef score_threshold_mask(\n    scores: np.ndarray,\n    labels: np.ndarray,\n    default_threshold: float,\n    class_thresholds: Mapping[int, float],\n) -> np.ndarray:\n    \"\"\"Return a boolean mask keeping predictions that pass per-class thresholds.\"\"\"\n\n    if scores.size == 0:\n        return np.zeros_like(scores, dtype=bool)\n\n    labels_int = labels.astype(np.int64, copy=False)\n    thresholds = np.full(scores.shape, default_threshold, dtype=scores.dtype)\n    if class_thresholds:\n        labels_fg = labels_int - 1  # convert to original 0-based ids\n        for cls, value in class_thresholds.items():\n            thresholds[labels_fg == int(cls)] = float(value)\n\n    keep = labels_int != 0  # drop background predictions outright\n    keep &= scores >= thresholds\n    return keep\n\n\ndef parse_class_threshold_entries(entries: Sequence[str]) -> Dict[int, float]:\n    \"\"\"Parse ``CLS=THRESH`` strings into a mapping of per-class thresholds.\"\"\"\n\n    thresholds: Dict[int, float] = {}\n    for entry in entries:\n        if not entry:\n            continue\n\n        if \"=\" in entry:\n            key, value = entry.split(\"=\", 1)\n        elif \":\" in entry:\n            key, value = entry.split(\":\", 1)\n        else:\n            raise ValueError(f\"Invalid class threshold format: {entry!r}\")\n\n        key = key.strip()\n        value = value.strip()\n        if not key or not value:\n            raise ValueError(f\"Invalid class threshold entry: {entry!r}\")\n\n        thresholds[int(key)] = float(value)\n\n    return thresholds\n\n\ndef running_in_ipython_kernel() -> bool:\n    \"\"\"Return ``True`` when executing inside an IPython/Jupyter kernel.\"\"\"\n\n    try:  # ``IPython`` may be absent in some execution environments.\n        from IPython import get_ipython  # type: ignore\n    except Exception:  # pragma: no cover - depends on environment\n        return False\n\n    shell = get_ipython()\n    return bool(shell and getattr(shell, \"kernel\", None))\n\n\ndef emit_metric_lines(\n    lines: Sequence[str],\n    *,\n    logger: Optional[logging.Logger] = None,\n    force_print: Optional[bool] = None,\n) -> None:\n    \"\"\"Log metric lines and optionally echo them to ``stdout``.\n\n    Kaggle notebooks buffer ``logging`` output differently from regular Python\n    scripts, so we proactively mirror the messages with ``print`` when we detect\n    an IPython kernel.  Callers may override this behaviour by passing\n    ``force_print`` explicitly.\n    \"\"\"\n\n    if logger is None:\n        logger = logging.getLogger(__name__)\n\n    should_print = force_print if force_print is not None else running_in_ipython_kernel()\n\n    for line in lines:\n        if logger is not None:\n            logger.info(line)\n        if should_print:\n            print(line)\n\n\ndef _resolve_class_label(dataset_cfg: DatasetConfig, index: int) -> str:\n    if index < len(dataset_cfg.class_names):\n        label = dataset_cfg.class_names[index]\n    else:\n        label = f\"class_{index:02d}\"\n\n    if label.startswith(\"class_\") and label[6:].isdigit():\n        return f\"class {int(label[6:]):02d}\"\n    return label\n\n\ndef format_epoch_metrics(\n    epoch: Optional[int],\n    train_loss: Optional[float],\n    metrics: Dict[str, torch.Tensor | float | List[float]],\n    dataset_cfg: DatasetConfig,\n    *,\n    header: Optional[str] = None,\n) -> List[str]:\n    lines: List[str] = []\n\n    val_loss = float(metrics.get(\"loss\", float(\"nan\")))\n    map_value = float(metrics.get(\"mAP\", float(\"nan\")))\n\n    if header is not None:\n        summary = header\n    elif epoch is not None:\n        summary = f\"Epoch {epoch:02d}\"\n    else:\n        summary = \"Metrics\"\n\n    if train_loss is not None and np.isfinite(train_loss):\n        summary += f\" | train loss {train_loss:.4f}\"\n    if np.isfinite(val_loss):\n        summary += f\" | val loss {val_loss:.4f}\"\n    if np.isfinite(map_value):\n        summary += f\" | mAP {map_value:.4f}\"\n    lines.append(summary)\n\n    precision = np.asarray(metrics.get(\"precision\", []), dtype=float)\n    recall = np.asarray(metrics.get(\"recall\", []), dtype=float)\n    tp = np.asarray(metrics.get(\"TP\", []), dtype=int)\n    fp = np.asarray(metrics.get(\"FP\", []), dtype=int)\n    fn = np.asarray(metrics.get(\"FN\", []), dtype=int)\n    ap = np.asarray(metrics.get(\"AP\", []), dtype=float)\n    gt_counter = np.asarray(metrics.get(\"gt_counter\", np.zeros_like(tp)), dtype=int)\n\n    num_classes = min(len(tp), dataset_cfg.num_classes)\n    for cls_idx in range(num_classes):\n        gt_value = int(gt_counter[cls_idx]) if gt_counter.size > cls_idx else 0\n        tp_value = int(tp[cls_idx]) if tp.size > cls_idx else 0\n        fp_value = int(fp[cls_idx]) if fp.size > cls_idx else 0\n        fn_value = int(fn[cls_idx]) if fn.size > cls_idx else 0\n\n        if gt_value == 0 and tp_value == 0 and fp_value == 0 and fn_value == 0:\n            continue\n\n        label = _resolve_class_label(dataset_cfg, cls_idx)\n        p_val = (\n            float(np.nan_to_num(precision[cls_idx], nan=0.0))\n            if precision.size > cls_idx\n            else 0.0\n        )\n        r_val = (\n            float(np.nan_to_num(recall[cls_idx], nan=0.0))\n            if recall.size > cls_idx\n            else 0.0\n        )\n        line = f\"{label} | P={p_val:.3f} R={r_val:.3f}  TP={tp_value} FP={fp_value} FN={fn_value}\"\n        if ap.size > cls_idx and np.isfinite(ap[cls_idx]):\n            line += f\" AP={ap[cls_idx]:.3f}\"\n        lines.append(line)\n\n    return lines\n\n\ndef load_default_font() -> ImageFont.FreeTypeFont | ImageFont.ImageFont:\n    \"\"\"Load a truetype font when available, otherwise fall back to default.\"\"\"\n\n    try:\n        return ImageFont.truetype(\"DejaVuSans.ttf\", size=14)\n    except Exception:  # pragma: no cover - fallback when font unavailable\n        return ImageFont.load_default()\n\n\ndef _resolve_class_name(class_names: Sequence[str], label: int) -> str:\n    if 0 <= label < len(class_names):\n        name = class_names[label]\n    else:\n        name = f\"class_{label:02d}\"\n\n    if name.startswith(\"class_\") and name[6:].isdigit():\n        return f\"class {int(name[6:]):02d}\"\n    return name\n\n\ndef render_detections(\n    image: np.ndarray,\n    prediction: Mapping[str, np.ndarray],\n    target: Mapping[str, np.ndarray] | None,\n    class_names: Sequence[str],\n    score_threshold: float,\n    class_thresholds: Mapping[int, float],\n    draw_ground_truth: bool,\n) -> PILImage:\n    \"\"\"Render detection predictions (and optional ground truth) onto an image.\"\"\"\n\n    if image.dtype != np.uint8:\n        image_array = np.clip(image, 0, 255).astype(np.uint8)\n    else:\n        image_array = image\n\n    pil = PILImage.fromarray(image_array)\n    draw = ImageDraw.Draw(pil)\n    font = load_default_font()\n\n    boxes = np.asarray(prediction.get(\"boxes\", np.empty((0, 4))), dtype=float)\n    labels = np.asarray(prediction.get(\"labels\", np.empty((0,), dtype=int)), dtype=int)\n    scores = np.asarray(prediction.get(\"scores\", np.empty((0,), dtype=float)), dtype=float)\n\n    for box, label, score in zip(boxes, labels, scores):\n        threshold = class_thresholds.get(int(label), score_threshold)\n        if score < threshold:\n            continue\n\n        color = DEFAULT_COLORS[int(label) % len(DEFAULT_COLORS)] if len(DEFAULT_COLORS) else \"#FF6B6B\"\n        x1, y1, x2, y2 = [float(coord) for coord in box]\n        draw.rectangle([x1, y1, x2, y2], outline=color, width=2)\n\n        caption = f\"{_resolve_class_name(class_names, int(label))} {score:.2f}\"\n        text_width = draw.textlength(caption, font=font)\n        draw.rectangle([x1, y1 - 16, x1 + text_width + 8, y1], fill=color)\n        draw.text((x1 + 4, y1 - 14), caption, fill=\"white\", font=font)\n\n    if draw_ground_truth and target is not None:\n        gt_boxes = np.asarray(target.get(\"boxes\", np.empty((0, 4))), dtype=float)\n        gt_labels = np.asarray(target.get(\"labels\", np.empty((0,), dtype=int)), dtype=int)\n        for box, label in zip(gt_boxes, gt_labels):\n            x1, y1, x2, y2 = [float(coord) for coord in box]\n            draw.rectangle([x1, y1, x2, y2], outline=\"#FFFFFF\", width=1)\n            caption = f\"GT {_resolve_class_name(class_names, int(label))}\"\n            text_width = draw.textlength(caption, font=font)\n            draw.rectangle([x1, y2, x1 + text_width + 6, y2 + 14], fill=\"#FFFFFF\")\n            draw.text((x1 + 3, y2), caption, fill=\"black\", font=font)\n\n    return pil\n\n\ndef save_detection_visual(\n    image: np.ndarray,\n    prediction: Mapping[str, np.ndarray],\n    target: Mapping[str, np.ndarray] | None,\n    class_names: Sequence[str],\n    score_threshold: float,\n    class_thresholds: Mapping[int, float],\n    draw_ground_truth: bool,\n    output_path: Path,\n) -> None:\n    \"\"\"Render and persist a detection visualisation to ``output_path``.\"\"\"\n\n    visual = render_detections(\n        image,\n        prediction,\n        target,\n        class_names,\n        score_threshold,\n        class_thresholds,\n        draw_ground_truth,\n    )\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    visual.save(output_path)\n\n\ndef write_false_positive_report(\n    fp_records: Sequence[Dict[str, object]],\n    report_path: Path,\n    *,\n    split: str,\n    score_threshold: float,\n    class_score_thresholds: Mapping[int, float],\n    iou_threshold: float,\n) -> None:\n    \"\"\"Serialise detailed false-positive information to JSON.\"\"\"\n\n    report_payload = {\n        \"split\": split,\n        \"score_threshold\": score_threshold,\n        \"class_score_thresholds\": dict(class_score_thresholds),\n        \"iou_threshold\": iou_threshold,\n        \"false_positive_images\": list(fp_records),\n    }\n    report_path.parent.mkdir(parents=True, exist_ok=True)\n    report_path.write_text(json.dumps(report_payload, indent=2, ensure_ascii=False))\n\n\ndef write_false_positive_list(fp_records: Sequence[Dict[str, object]], list_path: Path) -> None:\n    \"\"\"Write newline separated image identifiers that triggered false positives.\"\"\"\n\n    stems = sorted({str(record[\"image_id\"]) for record in fp_records})\n    list_path.parent.mkdir(parents=True, exist_ok=True)\n    list_path.write_text(\"\\n\".join(stems) + (\"\\n\" if stems else \"\"))\n\n\nclass SmoothedValue:\n    \"\"\"Track a series of values and provide access to smoothed statistics.\"\"\"\n\n    def __init__(self, window_size: int = 20) -> None:\n        self.window_size = window_size\n        self.deque: List[float] = []\n        self.total = 0.0\n        self.count = 0\n\n    def update(self, value: float) -> None:\n        if len(self.deque) == self.window_size:\n            self.total -= self.deque.pop(0)\n        self.deque.append(value)\n        self.total += value\n        self.count += 1\n\n    @property\n    def avg(self) -> float:\n        if not self.deque:\n            return 0.0\n        return self.total / len(self.deque)\n\n\nclass MetricLogger:\n    \"\"\"Helper class that logs running averages for multiple metrics.\"\"\"\n\n    def __init__(self) -> None:\n        self.meters: Dict[str, SmoothedValue] = {}\n\n    def update(self, **kwargs: float) -> None:\n        for name, value in kwargs.items():\n            if name not in self.meters:\n                self.meters[name] = SmoothedValue()\n            self.meters[name].update(float(value))\n\n    def format(self) -> str:\n        parts = [f\"{name}: {meter.avg:.4f}\" for name, meter in self.meters.items()]\n        return \" | \".join(parts)\n","metadata":{"execution":{"iopub.status.busy":"2025-10-29T06:05:09.076655Z","iopub.execute_input":"2025-10-29T06:05:09.076878Z","iopub.status.idle":"2025-10-29T06:05:09.136220Z","shell.execute_reply.started":"2025-10-29T06:05:09.076859Z","shell.execute_reply":"2025-10-29T06:05:09.135530Z"},"papermill":{"duration":0.030954,"end_time":"2025-10-21T07:01:27.379919","exception":false,"start_time":"2025-10-21T07:01:27.348965","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":5},{"id":"aca99e12","cell_type":"markdown","source":"## Model construction\n","metadata":{"papermill":{"duration":0.003005,"end_time":"2025-10-21T07:01:27.386218","exception":false,"start_time":"2025-10-21T07:01:27.383213","status":"completed"},"tags":[]}},{"id":"f07e4d0c","cell_type":"code","source":"\"\"\"Model building utilities for Faster R-CNN based detectors.\"\"\"\n\nimport logging\nfrom pathlib import Path\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\nfrom torchvision.models.detection import FasterRCNN_ResNet50_FPN_V2_Weights\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.rpn import AnchorGenerator, RPNHead\n\n\n\nLOGGER = logging.getLogger(__name__)\n\n\ndef _save_state_dict(model: nn.Module, path: Path) -> None:\n    try:\n        torch.save(model.state_dict(), path)\n        LOGGER.info(\"Saved pretrained weights to %s\", path)\n    except Exception as exc:  # pragma: no cover - safety net\n        LOGGER.warning(\"Unable to save pretrained weights: %s\", exc)\n\n\ndef build_model(\n    dataset_cfg: DatasetConfig,\n    train_cfg: TrainingConfig,\n    device: Optional[torch.device] = None,\n) -> nn.Module:\n    \"\"\"Create a Faster R-CNN model adjusted for the project dataset.\"\"\"\n    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model = _load_pretrained_model(train_cfg)\n\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    num_classes_with_background = dataset_cfg.num_classes + 1\n    model.roi_heads.box_predictor = FastRCNNPredictor(\n        in_features, num_classes_with_background\n    )\n\n    if train_cfg.small_object:\n        anchor_generator = AnchorGenerator(\n            #sizes=((16,), (32,), (64,), (128,), (256,)),\n            #aspect_ratios=((0.5, 1.0, 2.0),) * 5,\n            sizes=((16, 24), (32, 48), (64, 96), (128, 192), (256, 384)),\n            aspect_ratios=((0.2, 0.5, 1.0, 2.0, 5.0),) * 5\n        )\n        model.rpn.anchor_generator = anchor_generator\n        LOGGER.info(\"Using custom anchor sizes optimised for small objects\")\n\n        # 1.  RPNHead  in_channels\n        in_channels = model.rpn.head.cls_logits.in_channels\n\n        # 2.  AnchorGenerator \n        #    (5  aspect_ratios * 2  sizes = 10)\n        num_anchors_per_location = anchor_generator.num_anchors_per_location()[0]\n    \n        # 3.  RPNHead\n        new_head = RPNHead(in_channels, num_anchors_per_location)\n        model.rpn.head = new_head\n        LOGGER.info(\n            \"Re-created RPN head for %d anchors per location to match AnchorGenerator.\",\n            num_anchors_per_location\n        )\n\n    model.to(device)\n    return model\n\n\ndef _load_pretrained_model(train_cfg: TrainingConfig) -> nn.Module:\n    \"\"\"Load a Faster R-CNN model with fallback to local weights when offline.\"\"\"\n    pretrained_path = train_cfg.pretrained_weights_path\n    pretrained_path.parent.mkdir(parents=True, exist_ok=True)\n\n    weights_enum = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n    try:\n        model = fasterrcnn_resnet50_fpn_v2(weights=weights_enum)\n        LOGGER.info(\"Loaded torchvision Faster R-CNN weights\")\n        if not pretrained_path.exists():\n            _save_state_dict(model, pretrained_path)\n        return model\n    except Exception:  # pragma: no cover - fallback when torchvision download fails\n        LOGGER.warning(\"Falling back to locally saved pretrained detector weights\")\n        if not pretrained_path.exists():\n            raise RuntimeError(\n                \"No pretrained weights available. Download the torchvision weights manually \"\n                \"and place them at %s\" % pretrained_path\n            )\n        state_dict = torch.load(pretrained_path, map_location=\"cpu\")\n        model = fasterrcnn_resnet50_fpn_v2(weights=None)\n        model.load_state_dict(state_dict)\n        return model\n","metadata":{"execution":{"iopub.status.busy":"2025-10-29T06:05:09.137171Z","iopub.execute_input":"2025-10-29T06:05:09.137426Z","iopub.status.idle":"2025-10-29T06:05:09.154255Z","shell.execute_reply.started":"2025-10-29T06:05:09.137400Z","shell.execute_reply":"2025-10-29T06:05:09.153484Z"},"papermill":{"duration":0.013596,"end_time":"2025-10-21T07:01:27.403015","exception":false,"start_time":"2025-10-21T07:01:27.389419","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"id":"fcb051f6","cell_type":"markdown","source":"## Training utilities\n","metadata":{"papermill":{"duration":0.003072,"end_time":"2025-10-21T07:01:27.409284","exception":false,"start_time":"2025-10-21T07:01:27.406212","status":"completed"},"tags":[]}},{"id":"820377d5","cell_type":"code","source":"\"\"\"Training script for the electrical component Faster R-CNN detector.\"\"\"\n\nimport argparse\nimport contextlib\nimport inspect\nimport json\nimport logging\nimport numpy as np\nfrom dataclasses import asdict\nfrom pathlib import Path\nfrom typing import Dict, List, Set\nimport pickle\nimport torch\nfrom torch import nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ntry:\n    from torch.amp import GradScaler  # PyTorch 2.1+\nexcept ImportError:  # pragma: no cover - compatibility path\n    from torch.cuda.amp import GradScaler  # type: ignore[attr-defined]\n\ntry:\n    from torch.serialization import add_safe_globals\nexcept ImportError:  # pragma: no cover - compatibility path\n    add_safe_globals = None\n\n\n\n\nLOGGER = logging.getLogger(\"train\")\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"--data-dir\", type=Path, default=DatasetConfig().base_dir)\n    parser.add_argument(\"--epochs\", type=int, default=20)\n    parser.add_argument(\"--batch-size\", type=int, default=4)\n    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"Learning rate\")\n    parser.add_argument(\"--weight-decay\", type=float, default=1e-4)\n    parser.add_argument(\"--num-workers\", type=int, default=TrainingConfig().num_workers)\n    parser.add_argument(\"--no-augmentation\", action=\"store_true\", help=\"Disable data augmentation\")\n    parser.add_argument(\n        \"--mosaic-prob\",\n        type=float,\n        default=TrainingConfig().mosaic_prob,\n        help=\"Probability of applying mosaic augmentation (four-image collage)\",\n    )\n    parser.add_argument(\n        \"--mixup-prob\",\n        type=float,\n        default=TrainingConfig().mixup_prob,\n        help=\"Probability of mixing an additional image into the current sample\",\n    )\n    parser.add_argument(\n        \"--mixup-alpha\",\n        type=float,\n        default=TrainingConfig().mixup_alpha,\n        help=\"Alpha parameter for the Beta distribution controlling MixUp strength\",\n    )\n    parser.add_argument(\n        \"--scale-jitter\", nargs=2, type=float, metavar=(\"MIN\", \"MAX\"), default=None,\n        help=\"Uniform scale jitter range applied to each image before flips/jitter\",\n    )\n    parser.add_argument(\n        \"--rotation-prob\",\n        type=float,\n        default=TrainingConfig().rotation_prob,\n        help=\"Probability of applying a random in-place rotation.\",\n    )\n    parser.add_argument(\n        \"--rotation-max-degrees\",\n        type=float,\n        default=TrainingConfig().rotation_max_degrees,\n        help=\"Maximum absolute angle in degrees for random rotations.\",\n    )\n    parser.add_argument(\n        \"--affine-prob\",\n        type=float,\n        default=TrainingConfig().affine_prob,\n        help=\"Probability of applying a random affine transform (translate/scale/shear).\",\n    )\n    parser.add_argument(\n        \"--affine-translate\",\n        nargs=2,\n        type=float,\n        metavar=(\"FRAC_X\", \"FRAC_Y\"),\n        default=None,\n        help=\"Maximum absolute translation as a fraction of image width/height.\",\n    )\n    parser.add_argument(\n        \"--affine-scale\",\n        nargs=2,\n        type=float,\n        metavar=(\"MIN\", \"MAX\"),\n        default=None,\n        help=\"Uniform scaling range applied during affine augmentation.\",\n    )\n    parser.add_argument(\n        \"--affine-shear\",\n        nargs=2,\n        type=float,\n        metavar=(\"SHEAR_X\", \"SHEAR_Y\"),\n        default=None,\n        help=\"Maximum absolute shear angles (degrees) for the affine augmentation.\",\n    )\n    parser.add_argument(\"--small-object\", action=\"store_true\", help=\"Use smaller RPN anchors\")\n    parser.add_argument(\"--score-threshold\", type=float, default=0.6)\n    parser.add_argument(\n        \"--class-threshold\",\n        action=\"append\",\n        default=[],\n        metavar=\"CLS=THRESH\",\n        help=\"Override per-class score thresholds (e.g. --class-threshold 3=0.8)\",\n    )\n    parser.add_argument(\"--iou-threshold\", type=float, default=0.5)\n    parser.add_argument(\"--no-amp\", action=\"store_true\", help=\"Disable automatic mixed precision\")\n    parser.add_argument(\"--eval-interval\", type=int, default=1)\n    parser.add_argument(\"--seed\", type=int, default=2024)\n    parser.add_argument(\"--checkpoint\", type=Path, default=TrainingConfig().checkpoint_path)\n    parser.add_argument(\"--pretrained-path\", type=Path, default=TrainingConfig().pretrained_weights_path)\n    parser.add_argument(\"--resume\", action=\"store_true\", help=\"Resume training from a saved checkpoint.\")\n    parser.add_argument(\n        \"--resume-path\",\n        type=Path,\n        default=None,\n        help=\"Optional path to the checkpoint used for resuming training. Defaults to the last-checkpoint file.\",\n    )\n    parser.add_argument(\"--log-every\", type=int, default=20)\n    parser.add_argument(\"--train-split\", default=DatasetConfig().train_split)\n    parser.add_argument(\"--valid-split\", default=DatasetConfig().valid_split)\n    parser.add_argument(\"--num-classes\", type=int, default=DatasetConfig().num_classes)\n    parser.add_argument(\n        \"--exclude-list\",\n        action=\"append\",\n        default=[],\n        type=Path,\n        help=\"Path(s) to files listing image stems to exclude from training (text or JSON).\",\n    )\n    parser.add_argument(\n        \"--exclude-sample\",\n        action=\"append\",\n        default=[],\n        help=\"Additional image stems to remove from the training split.\",\n    )\n    parser.add_argument(\n        \"--fp-dir\",\n        type=Path,\n        default=None,\n        help=\"Directory where false-positive visualisations from the final epoch are written.\",\n    )\n    parser.add_argument(\n        \"--fp-report\",\n        type=Path,\n        help=\"Optional path to write a JSON report describing final-epoch false positives.\",\n    )\n    parser.add_argument(\n        \"--fp-list\",\n        type=Path,\n        help=\"Optional path to write newline separated image stems containing final-epoch false positives.\",\n    )\n    parser.add_argument(\n        \"--fp-class\",\n        action=\"append\",\n        type=int,\n        default=[],\n        help=\"Restrict false-positive exports to specific classes (repeatable).\",\n    )\n    return parser.parse_args()\n\n\ndef _normalise_stem(value: str) -> str:\n    return Path(value).stem if value else value\n\n\ndef _load_exclusions_from_file(path: Path) -> List[str]:\n    try:\n        text = path.read_text(encoding=\"utf-8\")\n    except OSError as exc:\n        LOGGER.warning(\"Unable to read exclude list %s: %s\", path, exc)\n        return []\n\n    try:\n        payload = json.loads(text)\n    except json.JSONDecodeError:\n        lines = [line.strip() for line in text.splitlines() if line.strip()]\n        return [_normalise_stem(line) for line in lines]\n\n    stems: List[str] = []\n    if isinstance(payload, dict):\n        for key in (\"stems\", \"image_ids\", \"images\"):\n            if key in payload and isinstance(payload[key], list):\n                stems.extend(_normalise_stem(str(item)) for item in payload[key])\n        if \"false_positive_images\" in payload and isinstance(payload[\"false_positive_images\"], list):\n            for entry in payload[\"false_positive_images\"]:\n                if isinstance(entry, dict):\n                    if \"image_id\" in entry:\n                        stems.append(_normalise_stem(str(entry[\"image_id\"])))\n                    elif \"image\" in entry:\n                        stems.append(_normalise_stem(str(entry[\"image\"])))\n    elif isinstance(payload, list):\n        for entry in payload:\n            if isinstance(entry, dict):\n                if \"image_id\" in entry:\n                    stems.append(_normalise_stem(str(entry[\"image_id\"])))\n                elif \"image\" in entry:\n                    stems.append(_normalise_stem(str(entry[\"image\"])))\n            else:\n                stems.append(_normalise_stem(str(entry)))\n\n    return [stem for stem in stems if stem]\n\n\ndef _resolve_exclusions(paths: List[Path], samples: List[str]) -> List[str]:\n    stems: Set[str] = {_normalise_stem(sample) for sample in samples if sample}\n    for path in paths:\n        if path is None:\n            continue\n        if not path.exists():\n            LOGGER.warning(\"Exclude list %s does not exist; skipping.\", path)\n            continue\n        stems.update(_load_exclusions_from_file(path))\n    return sorted(stem for stem in stems if stem)\n\n\ndef prepare_configs(args: argparse.Namespace) -> tuple[DatasetConfig, TrainingConfig]:\n    default_train_cfg = TrainingConfig()\n    dataset_cfg = DatasetConfig(\n        base_dir=args.data_dir,\n        train_split=args.train_split,\n        valid_split=args.valid_split,\n        num_classes=args.num_classes,\n    )\n\n    class_thresholds = DEFAULT_CLASS_SCORE_THRESHOLDS.copy()\n    overrides = parse_class_threshold_entries(args.class_threshold)\n    class_thresholds.update(overrides)\n\n    exclude_samples = _resolve_exclusions(args.exclude_list, args.exclude_sample)\n\n    if args.fp_class:\n        fp_class_values = sorted({int(value) for value in args.fp_class})\n        fp_classes = tuple(fp_class_values)\n    else:\n        fp_classes = default_train_cfg.fp_classes\n\n    fp_dir = args.fp_dir if args.fp_dir is not None else default_train_cfg.fp_visual_dir\n\n    if args.scale_jitter:\n        scale_min, scale_max = args.scale_jitter\n    else:\n        scale_min = default_train_cfg.scale_jitter_min\n        scale_max = default_train_cfg.scale_jitter_max\n\n    rotation_prob = args.rotation_prob\n    rotation_max = args.rotation_max_degrees\n\n    affine_prob = args.affine_prob\n    if args.affine_translate is not None:\n        affine_translate = (float(args.affine_translate[0]), float(args.affine_translate[1]))\n    else:\n        affine_translate = default_train_cfg.affine_translate\n\n    if args.affine_scale is not None:\n        affine_scale_range = (float(args.affine_scale[0]), float(args.affine_scale[1]))\n    else:\n        affine_scale_range = default_train_cfg.affine_scale_range\n\n    if args.affine_shear is not None:\n        affine_shear = (float(args.affine_shear[0]), float(args.affine_shear[1]))\n    else:\n        affine_shear = default_train_cfg.affine_shear\n\n    last_checkpoint_path = args.checkpoint.parent / \"last_checkpoint.pth\"\n    if args.resume_path is not None:\n        resume_path = args.resume_path\n    elif args.resume:\n        resume_path = last_checkpoint_path\n    else:\n        resume_path = None\n\n    train_cfg = TrainingConfig(\n        epochs=args.epochs,\n        batch_size=args.batch_size,\n        learning_rate=args.lr,\n        weight_decay=args.weight_decay,\n        num_workers=args.num_workers,\n        amp=not args.no_amp,\n        augmentation=not args.no_augmentation,\n        mosaic_prob=args.mosaic_prob,\n        mixup_prob=args.mixup_prob,\n        mixup_alpha=args.mixup_alpha,\n        scale_jitter_min=scale_min,\n        scale_jitter_max=scale_max,\n        rotation_prob=rotation_prob,\n        rotation_max_degrees=rotation_max,\n        affine_prob=affine_prob,\n        affine_translate=affine_translate,\n        affine_scale_range=affine_scale_range,\n        affine_shear=affine_shear,\n        small_object=args.small_object,\n        score_threshold=args.score_threshold,\n        iou_threshold=args.iou_threshold,\n        eval_interval=args.eval_interval,\n        seed=args.seed,\n        checkpoint_path=args.checkpoint,\n        pretrained_weights_path=args.pretrained_path,\n        resume=args.resume,\n        resume_path=resume_path,\n        last_checkpoint_path=last_checkpoint_path,\n        log_every=args.log_every,\n        class_score_thresholds=class_thresholds,\n        exclude_samples=tuple(exclude_samples),\n        fp_visual_dir=fp_dir,\n        fp_report_path=args.fp_report,\n        fp_list_path=args.fp_list,\n        fp_classes=fp_classes,\n    )\n    train_cfg.ensure_directories()\n    return dataset_cfg, train_cfg\n\n\ndef move_to_device(targets: List[Dict[str, torch.Tensor]], device: torch.device) -> List[Dict[str, torch.Tensor]]:\n    return [{k: v.to(device) for k, v in target.items()} for target in targets]\n\n\ndef train_one_epoch(\n    model: nn.Module,\n    loader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    scaler: GradScaler,\n    device: torch.device,\n    amp: bool,\n    log_every: int,\n) -> float:\n    model.train()\n    metric_logger = MetricLogger()\n    progress = tqdm(loader, desc=\"Train\", leave=False)\n\n    for step, (images, targets) in enumerate(progress, start=1):\n        images = [img.to(device) for img in images]\n        targets = move_to_device(targets, device)\n\n        optimizer.zero_grad()\n        autocast_enabled = amp and device.type == \"cuda\"\n        autocast_context = (\n            torch.amp.autocast(device_type=\"cuda\") if autocast_enabled else contextlib.nullcontext()\n        )\n        with autocast_context:\n            loss_dict = model(images, targets)\n            loss = sum(loss_dict.values())\n\n        if torch.isfinite(loss):\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:  # pragma: no cover - guard for invalid losses\n            LOGGER.warning(\"Skipping step %s due to non-finite loss\", step)\n            scaler.update()\n            continue\n\n        metric_logger.update(loss=loss.item())\n        if step % log_every == 0:\n            progress.set_postfix_str(metric_logger.format())\n\n    return metric_logger.meters.get(\"loss\").avg if metric_logger.meters else 0.0\n\n\n@torch.no_grad()\ndef evaluate(\n    model: nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    dataset_cfg: DatasetConfig,\n    train_cfg: TrainingConfig,\n    *,\n    dataset: ElectricalComponentsDataset | None = None,\n    collect_details: bool = False,\n) -> tuple[Dict[str, torch.Tensor | float | List[float]], List[Dict[str, object]]]:\n    was_training = model.training\n    model.eval()\n\n    predictions = []\n    targets_for_eval = []\n    total_loss = 0.0\n    num_batches = 0\n\n    sample_details: List[Dict[str, object]] = []\n    dataset_ref = dataset if dataset is not None else getattr(loader, \"dataset\", None)\n\n    for images, targets in loader:\n        images = [img.to(device) for img in images]\n        targets_device = move_to_device(targets, device)\n\n        model.train()\n        loss_dict = model(images, targets_device)\n        total_loss += sum(loss_dict.values()).item()\n        num_batches += 1\n        model.eval()\n\n        outputs = model(images)\n        for output, target_device, target_cpu in zip(outputs, targets_device, targets):\n            boxes_np = output[\"boxes\"].detach().cpu().numpy()\n            scores_np = output[\"scores\"].detach().cpu().numpy()\n            labels_np = output[\"labels\"].detach().cpu().numpy().astype(np.int64, copy=False)\n            keep = score_threshold_mask(\n                scores_np,\n                labels_np,\n                train_cfg.score_threshold,\n                train_cfg.class_score_thresholds,\n            )\n            boxes_np = boxes_np[keep]\n            scores_np = scores_np[keep]\n            labels_np = labels_np[keep].astype(np.int64, copy=True)\n            if labels_np.size:\n                labels_np -= 1\n\n            target_boxes = target_device[\"boxes\"].detach().cpu().numpy()\n            target_labels = target_device[\"labels\"].detach().cpu().numpy().astype(np.int64, copy=True)\n            if target_labels.size:\n                gt_keep = target_labels > 0\n                target_boxes = target_boxes[gt_keep]\n                target_labels = target_labels[gt_keep] - 1\n\n            prediction_np = {\n                \"boxes\": boxes_np,\n                \"scores\": scores_np,\n                \"labels\": labels_np,\n            }\n            target_np = {\n                \"boxes\": target_boxes,\n                \"labels\": target_labels,\n            }\n\n            predictions.append(prediction_np)\n            targets_for_eval.append(target_np)\n\n            if collect_details:\n                image_identifier = target_cpu.get(\"image_id\", -1)\n                if isinstance(image_identifier, torch.Tensor):\n                    image_index = int(image_identifier.item())\n                else:\n                    try:\n                        image_index = int(image_identifier)\n                    except Exception:\n                        image_index = -1\n                if dataset_ref is not None and 0 <= image_index < len(getattr(dataset_ref, \"image_stems\", [])):\n                    image_id = dataset_ref.image_stems[image_index]\n                else:\n                    image_id = f\"{dataset_cfg.valid_split}_{len(sample_details):04d}\"\n\n                fp_info = identify_false_positive_predictions(\n                    prediction_np,\n                    target_np,\n                    dataset_cfg.num_classes,\n                    train_cfg.iou_threshold,\n                )\n\n                sample_details.append(\n                    {\n                        \"image_index\": image_index,\n                        \"image_id\": image_id,\n                        \"prediction\": prediction_np,\n                        \"target\": target_np,\n                        \"false_positives\": fp_info,\n                    }\n                )\n\n    metrics = compute_detection_metrics(\n        predictions, targets_for_eval, dataset_cfg.num_classes, train_cfg.iou_threshold\n    )\n    metrics[\"loss\"] = total_loss / max(num_batches, 1)\n\n    if was_training:\n        model.train()\n    return metrics, sample_details\n\n\ndef save_checkpoint(\n    *,\n    path: Path,\n    model: nn.Module,\n    optimizer: torch.optim.Optimizer | None,\n    scaler: GradScaler | None,\n    epoch: int,\n    best_map: float,\n    history: List[Dict[str, float]],\n    train_cfg: TrainingConfig,\n) -> None:\n    checkpoint = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict() if optimizer is not None else None,\n        \"scaler\": scaler.state_dict() if scaler is not None and getattr(scaler, \"is_enabled\", lambda: True)() else None,\n        \"epoch\": epoch,\n        \"best_map\": float(best_map),\n        \"history\": list(history),\n        \"config\": asdict(train_cfg),\n    }\n    torch.save(checkpoint, path)\n    LOGGER.info(\"Saved checkpoint to %s (epoch %d, best mAP %.4f)\", path, epoch, best_map)\n\n\ndef _load_checkpoint_file(path: Path, device: torch.device) -> object:\n    load_kwargs = {\"map_location\": device}\n    try:\n        return torch.load(path, **load_kwargs)\n    except pickle.UnpicklingError:\n        safe_objects: List[object] = []\n        if add_safe_globals is not None:\n            safe_objects.append(TrainingConfig)\n            try:\n                safe_objects.append(type(Path(\".\")))\n            except Exception:  # pragma: no cover - defensive\n                pass\n            add_safe_globals(safe_objects)\n        load_params = inspect.signature(torch.load).parameters\n        if \"weights_only\" in load_params:\n            load_kwargs[\"weights_only\"] = False\n        return torch.load(path, **load_kwargs)\n\n\ndef load_checkpoint(\n    *,\n    path: Path,\n    model: nn.Module,\n    optimizer: torch.optim.Optimizer | None,\n    scaler: GradScaler | None,\n    device: torch.device,\n) -> tuple[int, float, List[Dict[str, float]]]:\n    if not path.exists():\n        LOGGER.warning(\"Checkpoint %s does not exist; starting fresh.\", path)\n        return 1, -float(\"inf\"), []\n\n    checkpoint_obj = _load_checkpoint_file(path, device)\n\n    if isinstance(checkpoint_obj, dict) and \"model\" in checkpoint_obj:\n        model.load_state_dict(checkpoint_obj[\"model\"])\n        if optimizer is not None and checkpoint_obj.get(\"optimizer\") is not None:\n            optimizer.load_state_dict(checkpoint_obj[\"optimizer\"])\n        if scaler is not None and checkpoint_obj.get(\"scaler\") is not None:\n            try:\n                scaler.load_state_dict(checkpoint_obj[\"scaler\"])\n            except Exception as exc:  # pragma: no cover - fallback if scaler config changed\n                LOGGER.warning(\"Unable to load scaler state from %s: %s\", path, exc)\n        epoch = int(checkpoint_obj.get(\"epoch\", 0))\n        best_map = float(checkpoint_obj.get(\"best_map\", -float(\"inf\")))\n        history = checkpoint_obj.get(\"history\", [])\n        LOGGER.info(\n            \"Loaded checkpoint from %s (epoch %d, best mAP %.4f)\",\n            path,\n            epoch,\n            best_map,\n        )\n        return epoch + 1, best_map, list(history)\n\n    # Legacy checkpoint: assume it's a plain state dict.\n    model.load_state_dict(checkpoint_obj)\n    LOGGER.info(\"Loaded model weights from legacy checkpoint %s\", path)\n    return 1, -float(\"inf\"), []\n\n\ndef _create_grad_scaler(amp_enabled: bool, device: torch.device) -> GradScaler:\n    \"\"\"Create a GradScaler compatible with both legacy and new AMP APIs.\"\"\"\n    scaler_enabled = amp_enabled and device.type == \"cuda\"\n    init_params = inspect.signature(GradScaler.__init__).parameters\n    kwargs = {\"enabled\": scaler_enabled}\n    if \"device_type\" in init_params:\n        kwargs[\"device_type\"] = device.type\n    return GradScaler(**kwargs)\n\n\ndef export_false_positive_visuals(\n    *,\n    dataset: ElectricalComponentsDataset,\n    dataset_cfg: DatasetConfig,\n    train_cfg: TrainingConfig,\n    sample_details: List[Dict[str, object]],\n) -> List[Dict[str, object]]:\n    \"\"\"Export false-positive visuals for the configured classes and return records.\"\"\"\n\n    if not train_cfg.fp_visual_dir:\n        return []\n\n    relevant_classes = {int(cls) for cls in train_cfg.fp_classes}\n    if not relevant_classes:\n        return []\n\n    output_dir = train_cfg.fp_visual_dir\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Keep only the last-epoch visuals by clearing previous outputs.\n    for existing in output_dir.glob(\"*.png\"):\n        try:\n            existing.unlink()\n        except OSError:\n            LOGGER.debug(\"Unable to remove previous FP visual %s\", existing)\n\n    fp_records: List[Dict[str, object]] = []\n\n    for detail in sample_details:\n        raw_records = detail.get(\"false_positives\", [])\n        if not raw_records:\n            continue\n\n        filtered = [fp for fp in raw_records if int(fp.get(\"class\", -1)) in relevant_classes]\n        if not filtered:\n            continue\n\n        image_id = str(detail.get(\"image_id\", detail.get(\"image_index\", \"unknown\")))\n        try:\n            image_np, _, _ = dataset._load_raw_sample(image_id)\n        except Exception as exc:  # pragma: no cover - defensive\n            LOGGER.warning(\"Skipping FP visual for %s due to load error: %s\", image_id, exc)\n            continue\n\n        output_path = output_dir / f\"{image_id}.png\"\n        save_detection_visual(\n            image_np,\n            detail.get(\"prediction\", {}),\n            detail.get(\"target\", {}),\n            dataset_cfg.class_names,\n            train_cfg.score_threshold,\n            train_cfg.class_score_thresholds,\n            True,\n            output_path,\n        )\n        fp_records.append({\"image_id\": image_id, \"false_positives\": filtered})\n\n    return fp_records\n\n\ndef run_training(args: argparse.Namespace) -> None:\n    dataset_cfg, train_cfg = prepare_configs(args)\n\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n    set_seed(train_cfg.seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = build_model(dataset_cfg, train_cfg, device=device)\n\n    train_dataset = ElectricalComponentsDataset(\n        root=dataset_cfg.base_dir,\n        split=dataset_cfg.train_split,\n        class_names=dataset_cfg.class_names,\n        transform=AugmentationParams(\n            mosaic_prob=train_cfg.mosaic_prob,\n            mixup_prob=train_cfg.mixup_prob,\n            mixup_alpha=train_cfg.mixup_alpha,\n            scale_jitter_range=(train_cfg.scale_jitter_min, train_cfg.scale_jitter_max),\n            rotation_prob=train_cfg.rotation_prob,\n            rotation_max_degrees=train_cfg.rotation_max_degrees,\n            affine_prob=train_cfg.affine_prob,\n            affine_translate=train_cfg.affine_translate,\n            affine_scale_range=train_cfg.affine_scale_range,\n            affine_shear=train_cfg.affine_shear,\n        ),\n        use_augmentation=train_cfg.augmentation,\n        exclude_stems=train_cfg.exclude_samples,\n    )\n    valid_dataset = ElectricalComponentsDataset(\n        root=dataset_cfg.base_dir,\n        split=dataset_cfg.valid_split,\n        class_names=dataset_cfg.class_names,\n        use_augmentation=False,\n    )\n\n    if train_cfg.exclude_samples:\n        sample_preview = \", \".join(train_cfg.exclude_samples[:5])\n        if len(train_cfg.exclude_samples) > 5:\n            sample_preview += \", ...\"\n        LOGGER.info(\n            \"Excluding %d training samples: %s\",\n            len(train_cfg.exclude_samples),\n            sample_preview,\n        )\n\n    train_loader = create_data_loaders(\n        train_dataset,\n        batch_size=train_cfg.batch_size,\n        shuffle=True,\n        num_workers=train_cfg.num_workers,\n    )\n    if train_cfg.num_workers > 1:\n        valid_workers = max(1, train_cfg.num_workers // 2)\n    else:\n        valid_workers = train_cfg.num_workers\n\n    valid_loader = create_data_loaders(\n        valid_dataset,\n        batch_size=train_cfg.batch_size,\n        shuffle=False,\n        num_workers=valid_workers,\n    )\n\n    optimizer = AdamW(model.parameters(), lr=train_cfg.learning_rate, weight_decay=train_cfg.weight_decay)\n    scaler = _create_grad_scaler(train_cfg.amp, device)\n\n    start_epoch = 1\n    best_map = -float(\"inf\")\n    history: List[Dict[str, float]] = []\n\n    if train_cfg.resume or train_cfg.resume_path is not None:\n        resume_source = train_cfg.resume_path or train_cfg.last_checkpoint_path\n        print(resume_source)\n        start_epoch, best_map, history = load_checkpoint(\n            path=resume_source,\n            model=model,\n            optimizer=optimizer,\n            scaler=scaler,\n            device=device,\n        )\n\n    for epoch in range(start_epoch, train_cfg.epochs + 1):\n        LOGGER.info(\"Epoch %s/%s\", epoch, train_cfg.epochs)\n        train_loss = train_one_epoch(\n            model, train_loader, optimizer, scaler, device, train_cfg.amp, train_cfg.log_every\n        )\n\n        should_evaluate = (epoch % train_cfg.eval_interval == 0) or (epoch == train_cfg.epochs)\n        collect_fp = should_evaluate and epoch == train_cfg.epochs\n\n        if should_evaluate:\n            metrics, sample_details = evaluate(\n                model,\n                valid_loader,\n                device,\n                dataset_cfg,\n                train_cfg,\n                dataset=valid_dataset,\n                collect_details=collect_fp,\n            )\n            metric_lines = format_epoch_metrics(epoch, train_loss, metrics, dataset_cfg)\n            emit_metric_lines(metric_lines, logger=LOGGER)\n\n            history.append(\n                {\n                    \"epoch\": epoch,\n                    \"train_loss\": float(train_loss),\n                    \"val_loss\": float(metrics[\"loss\"]),\n                    \"mAP\": float(metrics[\"mAP\"]),\n                }\n            )\n\n            if metrics[\"mAP\"] > best_map:\n                best_map = float(metrics[\"mAP\"])\n                save_checkpoint(\n                    path=train_cfg.checkpoint_path,\n                    model=model,\n                    optimizer=optimizer,\n                    scaler=scaler,\n                    epoch=epoch,\n                    best_map=best_map,\n                    history=history,\n                    train_cfg=train_cfg,\n                )\n\n            if collect_fp:\n                fp_records = export_false_positive_visuals(\n                    dataset=valid_dataset,\n                    dataset_cfg=dataset_cfg,\n                    train_cfg=train_cfg,\n                    sample_details=sample_details,\n                )\n                if train_cfg.fp_report_path:\n                    write_false_positive_report(\n                        fp_records,\n                        train_cfg.fp_report_path,\n                        split=dataset_cfg.valid_split,\n                        score_threshold=train_cfg.score_threshold,\n                        class_score_thresholds=train_cfg.class_score_thresholds,\n                        iou_threshold=train_cfg.iou_threshold,\n                    )\n                    LOGGER.info(\n                        \"Saved false-positive report for %d images to %s\",\n                        len(fp_records),\n                        train_cfg.fp_report_path,\n                    )\n                if train_cfg.fp_list_path:\n                    write_false_positive_list(fp_records, train_cfg.fp_list_path)\n                    LOGGER.info(\n                        \"Saved list of %d false-positive image ids to %s\",\n                        len({str(record[\"image_id\"]) for record in fp_records}),\n                        train_cfg.fp_list_path,\n                    )\n        else:\n            LOGGER.info(\n                \"Epoch %02d | train loss %.4f | evaluation skipped (eval_interval=%d)\",\n                epoch,\n                train_loss,\n                train_cfg.eval_interval,\n            )\n\n        save_checkpoint(\n            path=train_cfg.last_checkpoint_path,\n            model=model,\n            optimizer=optimizer,\n            scaler=scaler,\n            epoch=epoch,\n            best_map=best_map,\n            history=history,\n            train_cfg=train_cfg,\n        )\n\n    (train_cfg.output_dir / \"training_history.json\").write_text(json.dumps(history, indent=2))\n    LOGGER.info(\"Training complete. Best mAP: %.4f\", best_map)\n\n\ndef main() -> None:\n    args = parse_args()\n    run_training(args)\n","metadata":{"execution":{"iopub.status.busy":"2025-10-29T06:05:09.155351Z","iopub.execute_input":"2025-10-29T06:05:09.155692Z","iopub.status.idle":"2025-10-29T06:05:09.220058Z","shell.execute_reply.started":"2025-10-29T06:05:09.155674Z","shell.execute_reply":"2025-10-29T06:05:09.219442Z"},"papermill":{"duration":0.029234,"end_time":"2025-10-21T07:01:27.441714","exception":false,"start_time":"2025-10-21T07:01:27.412480","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":7},{"id":"c7d6760e","cell_type":"markdown","source":"## Inference helpers\n","metadata":{"papermill":{"duration":0.003126,"end_time":"2025-10-21T07:01:27.448074","exception":false,"start_time":"2025-10-21T07:01:27.444948","status":"completed"},"tags":[]}},{"id":"a7f63ef8","cell_type":"code","source":"\"\"\"Inference utilities for evaluating trained models on the test split.\"\"\"\n\nimport argparse\nimport logging\nimport pickle\nimport inspect\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ntry:\n    from torch.serialization import add_safe_globals\nexcept ImportError:  # pragma: no cover - compatibility for older PyTorch\n    add_safe_globals = None\n\n\n\n\nLOGGER = logging.getLogger(\"inference\")\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"--data-dir\", type=Path, default=DatasetConfig().base_dir)\n    parser.add_argument(\"--split\", default=DatasetConfig().test_split)\n    parser.add_argument(\"--checkpoint\", type=Path, required=True)\n    parser.add_argument(\"--output-dir\", type=Path, default=InferenceConfig().output_dir)\n    parser.add_argument(\"--score-threshold\", type=float, default=InferenceConfig().score_threshold)\n    parser.add_argument(\n        \"--class-threshold\",\n        action=\"append\",\n        default=[],\n        metavar=\"CLS=THRESH\",\n        help=\"Override per-class score thresholds for inference\",\n    )\n    parser.add_argument(\"--iou-threshold\", type=float, default=0.5)\n    parser.add_argument(\"--max-images\", type=int, default=InferenceConfig().max_images)\n    parser.add_argument(\"--num-classes\", type=int, default=DatasetConfig().num_classes)\n    parser.add_argument(\"--draw-ground-truth\", action=\"store_true\", default=False)\n    parser.add_argument(\"--seed\", type=int, default=2024)\n    parser.add_argument(\"--pretrained-path\", type=Path, default=TrainingConfig().pretrained_weights_path)\n    parser.add_argument(\n        \"--fp-report\",\n        type=Path,\n        help=\"Optional path to write a JSON report of images containing false positives.\",\n    )\n    parser.add_argument(\n        \"--fp-list\",\n        type=Path,\n        help=\"Optional path to write newline separated image stems that produced false positives.\",\n    )\n    return parser.parse_args()\n\n\ndef _load_checkpoint_state(path: Path, device: torch.device) -> Dict[str, torch.Tensor]:\n    load_kwargs = {\"map_location\": device}\n    try:\n        checkpoint_obj = torch.load(path, **load_kwargs)\n    except pickle.UnpicklingError:\n        if add_safe_globals is not None:\n            add_safe_globals([TrainingConfig])\n        load_params = inspect.signature(torch.load).parameters\n        if \"weights_only\" in load_params:\n            load_kwargs[\"weights_only\"] = False\n        checkpoint_obj = torch.load(path, **load_kwargs)\n\n    if isinstance(checkpoint_obj, dict) and \"model\" in checkpoint_obj:\n        return checkpoint_obj[\"model\"]\n\n    return checkpoint_obj\n@torch.no_grad()\ndef run_inference(args: argparse.Namespace) -> None:\n    dataset_cfg = DatasetConfig(\n        base_dir=args.data_dir,\n        test_split=args.split,\n        num_classes=args.num_classes,\n    )\n    class_thresholds = DEFAULT_CLASS_SCORE_THRESHOLDS.copy()\n    overrides = parse_class_threshold_entries(args.class_threshold)\n    class_thresholds.update(overrides)\n\n    inference_cfg = InferenceConfig(\n        score_threshold=args.score_threshold,\n        max_images=args.max_images,\n        output_dir=args.output_dir,\n        draw_ground_truth=args.draw_ground_truth,\n        class_score_thresholds=class_thresholds,\n    )\n    inference_cfg.ensure_directories()\n\n    train_cfg = TrainingConfig(\n        augmentation=False,\n        score_threshold=args.score_threshold,\n        iou_threshold=args.iou_threshold,\n        pretrained_weights_path=args.pretrained_path,\n        class_score_thresholds=class_thresholds,\n    )\n\n    set_seed(args.seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = build_model(dataset_cfg, train_cfg, device=device)\n    state_dict = _load_checkpoint_state(args.checkpoint, device)\n    model.load_state_dict(state_dict)\n    model.eval()\n\n    dataset = ElectricalComponentsDataset(\n        root=dataset_cfg.base_dir,\n        split=args.split,\n        class_names=dataset_cfg.class_names,\n        use_augmentation=False,\n    )\n    loader = create_data_loaders(dataset, batch_size=1, shuffle=False, num_workers=0)\n\n    predictions: List[Dict[str, np.ndarray]] = []\n    targets_for_eval: List[Dict[str, np.ndarray]] = []\n\n    progress = tqdm(loader, desc=\"Infer\")\n    fp_records: List[Dict[str, object]] = []\n    for idx, (images, targets) in enumerate(progress):\n        image = images[0].to(device)\n        output = model([image])[0]\n\n        boxes_np = output[\"boxes\"].detach().cpu().numpy()\n        scores_np = output[\"scores\"].detach().cpu().numpy()\n        labels_np = output[\"labels\"].detach().cpu().numpy().astype(np.int64, copy=False)\n        keep = score_threshold_mask(\n            scores_np,\n            labels_np,\n            inference_cfg.score_threshold,\n            inference_cfg.class_score_thresholds,\n        )\n        boxes_np = boxes_np[keep]\n        scores_np = scores_np[keep]\n        labels_np = labels_np[keep].astype(np.int64, copy=True)\n        if labels_np.size:\n            labels_np -= 1\n\n        target_boxes = targets[0][\"boxes\"].detach().cpu().numpy()\n        target_labels = targets[0][\"labels\"].detach().cpu().numpy().astype(np.int64, copy=True)\n        if target_labels.size:\n            gt_keep = target_labels > 0\n            target_boxes = target_boxes[gt_keep]\n            target_labels = target_labels[gt_keep] - 1\n\n        prediction_np = {\n            \"boxes\": boxes_np,\n            \"scores\": scores_np,\n            \"labels\": labels_np,\n        }\n        target_np = {\n            \"boxes\": target_boxes,\n            \"labels\": target_labels,\n        }\n\n        predictions.append(prediction_np)\n        targets_for_eval.append(target_np)\n\n        fp_details = identify_false_positive_predictions(\n            prediction_np,\n            target_np,\n            dataset_cfg.num_classes,\n            args.iou_threshold,\n        )\n        if fp_details:\n            image_id = dataset.image_stems[idx] if idx < len(dataset.image_stems) else f\"{args.split}_{idx:04d}\"\n            fp_records.append(\n                {\n                    \"image_id\": image_id,\n                    \"false_positives\": fp_details,\n                }\n            )\n\n        if idx < inference_cfg.max_images:\n            image_np = (images[0].permute(1, 2, 0).numpy() * 255.0).astype(np.uint8)\n            output_path = inference_cfg.output_dir / f\"{args.split}_{idx:04d}.png\"\n            save_detection_visual(\n                image_np,\n                prediction_np,\n                target_np if args.draw_ground_truth else None,\n                dataset_cfg.class_names,\n                inference_cfg.score_threshold,\n                inference_cfg.class_score_thresholds,\n                args.draw_ground_truth,\n                output_path,\n            )\n\n    metrics = compute_detection_metrics(\n        predictions, targets_for_eval, dataset_cfg.num_classes, args.iou_threshold\n    )\n    metric_lines = format_epoch_metrics(\n        epoch=None,\n        train_loss=None,\n        metrics=metrics,\n        dataset_cfg=dataset_cfg,\n        header=f\"Inference @ IoU {args.iou_threshold:.2f}\",\n    )\n    emit_metric_lines(metric_lines, logger=LOGGER)\n\n    if args.fp_report:\n        write_false_positive_report(\n            fp_records,\n            args.fp_report,\n            split=args.split,\n            score_threshold=inference_cfg.score_threshold,\n            class_score_thresholds=inference_cfg.class_score_thresholds,\n            iou_threshold=args.iou_threshold,\n        )\n        LOGGER.info(\n            \"Wrote false-positive report for %d images to %s\",\n            len(fp_records),\n            args.fp_report,\n        )\n\n    if args.fp_list:\n        write_false_positive_list(fp_records, args.fp_list)\n        LOGGER.info(\n            \"Wrote %d image ids with false positives to %s\",\n            len({str(record[\"image_id\"]) for record in fp_records}),\n            args.fp_list,\n        )\n\n\ndef main() -> None:\n    args = parse_args()\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n    run_inference(args)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-10-29T06:05:09.220995Z","iopub.execute_input":"2025-10-29T06:05:09.221812Z","iopub.status.idle":"2025-10-29T06:05:09.244997Z","shell.execute_reply.started":"2025-10-29T06:05:09.221781Z","shell.execute_reply":"2025-10-29T06:05:09.244402Z"},"papermill":{"duration":0.020147,"end_time":"2025-10-21T07:01:27.471429","exception":false,"start_time":"2025-10-21T07:01:27.451282","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":8},{"id":"1e7eab67","cell_type":"markdown","source":"## Example usage\n","metadata":{"papermill":{"duration":0.003132,"end_time":"2025-10-21T07:01:27.477944","exception":false,"start_time":"2025-10-21T07:01:27.474812","status":"completed"},"tags":[]}},{"id":"0ed17aed","cell_type":"markdown","source":"1. \n2.  `train_args` / `inference_args` `run_training(train_args)`  `run_inference(inference_args)`\n3.  `outputs/best_model.pth` epoch  `outputs/last_checkpoint.pth` `InferenceConfig.output_dir`\n---\nTuning History\n1. lr\n2. 0162530\n3. ((0.2, 0.5, 1.0, 2.0, 5.0),) * len(anchor_sizes)\n4. epoch\n5. 162530\n6. shuffle dataset\n7.  2530\n8. RPN","metadata":{"papermill":{"duration":0.002928,"end_time":"2025-10-21T07:01:27.483932","exception":false,"start_time":"2025-10-21T07:01:27.481004","status":"completed"},"tags":[]}},{"id":"cb8be183","cell_type":"code","source":"# Example usage inside the notebook\n# 1) Configure training arguments\ntrain_args = argparse.Namespace(\n    data_dir=Path('/kaggle/input/electrical-component/dataset_1021/dataset'),\n    epochs=85,\n    batch_size=2,\n    lr=5e-5,\n    weight_decay=5e-5,\n    num_workers=2,\n    no_augmentation=True,\n    mosaic_prob=TrainingConfig().mosaic_prob,\n    mixup_prob=TrainingConfig().mixup_prob,\n    mixup_alpha=TrainingConfig().mixup_alpha,\n    scale_jitter=None,\n    rotation_prob=TrainingConfig().rotation_prob,\n    rotation_max_degrees=TrainingConfig().rotation_max_degrees,\n    affine_prob=TrainingConfig().affine_prob,\n    affine_translate=None,\n    affine_scale=None,\n    affine_shear=None,\n    small_object=True,\n    score_threshold=0.6,\n    class_threshold=[],\n    iou_threshold=0.5,\n    no_amp=False,\n    eval_interval=1,\n    seed=37,\n    checkpoint=Path('/kaggle/working/outputs/best_model.pth'),\n    pretrained_path=TrainingConfig().pretrained_weights_path,\n    resume=None,\n    resume_path=Path('/kaggle/input/ecd-best-model-pth/pytorch/default/1/best_model.pth'),\n    log_every=20,\n    train_split=DatasetConfig().train_split,\n    valid_split=DatasetConfig().valid_split,\n    num_classes=DatasetConfig().num_classes,\n    exclude_list=[],\n    exclude_sample=[],\n    fp_dir=None,\n    fp_report=None,\n    fp_list=None,\n    fp_class=[],\n)\n\n# 2) Launch training (writes checkpoints to outputs/)\nrun_training(train_args)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-10-29T06:05:09.245831Z","iopub.execute_input":"2025-10-29T06:05:09.246065Z","iopub.status.idle":"2025-10-29T06:30:07.446276Z","shell.execute_reply.started":"2025-10-29T06:05:09.246047Z","shell.execute_reply":"2025-10-29T06:30:07.445626Z"},"papermill":{"duration":7794.040395,"end_time":"2025-10-21T09:11:21.527487","exception":false,"start_time":"2025-10-21T07:01:27.487092","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n100%|| 167M/167M [00:00<00:00, 183MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/ecd-best-model-pth/pytorch/default/1/best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch 81 | train loss 0.1798 | val loss 0.2443 | mAP 0.9388\nclass 00 | P=0.911 R=1.000  TP=41 FP=4 FN=0 AP=0.986\nclass 01 | P=0.919 R=0.971  TP=34 FP=3 FN=1 AP=0.982\nclass 02 | P=0.971 R=0.943  TP=33 FP=1 FN=2 AP=0.960\nclass 04 | P=1.000 R=0.963  TP=26 FP=0 FN=1 AP=0.981\nclass 05 | P=0.931 R=0.893  TP=67 FP=5 FN=8 AP=0.935\nclass 06 | P=0.714 R=1.000  TP=5 FP=2 FN=0 AP=0.995\nclass 07 | P=0.855 R=0.819  TP=59 FP=10 FN=13 AP=0.811\nclass 09 | P=0.912 R=1.000  TP=31 FP=3 FN=0 AP=0.995\nclass 10 | P=1.000 R=1.000  TP=45 FP=0 FN=0 AP=0.995\nclass 11 | P=1.000 R=0.968  TP=30 FP=0 FN=1 AP=0.984\nclass 13 | P=0.940 R=0.918  TP=78 FP=5 FN=7 AP=0.949\nclass 14 | P=0.883 R=0.892  TP=83 FP=11 FN=10 AP=0.905\nclass 15 | P=0.968 R=0.968  TP=30 FP=1 FN=1 AP=0.982\nclass 16 | P=1.000 R=1.000  TP=6 FP=0 FN=0 AP=0.995\nclass 18 | P=0.946 R=0.946  TP=35 FP=2 FN=2 AP=0.971\nclass 19 | P=0.933 R=0.903  TP=28 FP=2 FN=3 AP=0.939\nclass 20 | P=0.933 R=0.933  TP=14 FP=1 FN=1 AP=0.964\nclass 21 | P=0.600 R=0.429  TP=3 FP=2 FN=4 AP=0.564\nclass 22 | P=1.000 R=1.000  TP=2 FP=0 FN=0 AP=0.995\nclass 23 | P=1.000 R=1.000  TP=48 FP=0 FN=0 AP=0.995\nclass 25 | P=1.000 R=1.000  TP=3 FP=0 FN=0 AP=0.995\nclass 27 | P=1.000 R=1.000  TP=45 FP=0 FN=0 AP=0.995\nclass 28 | P=0.936 R=0.880  TP=44 FP=3 FN=6 AP=0.934\nclass 29 | P=0.792 R=0.955  TP=42 FP=11 FN=2 AP=0.953\nclass 30 | P=0.571 R=1.000  TP=4 FP=3 FN=0 AP=0.745\nclass 31 | P=0.820 R=0.924  TP=73 FP=16 FN=6 AP=0.904\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch 82 | train loss 0.1463 | val loss 0.2445 | mAP 0.9264\nclass 00 | P=0.854 R=1.000  TP=41 FP=7 FN=0 AP=0.990\nclass 01 | P=0.919 R=0.971  TP=34 FP=3 FN=1 AP=0.967\nclass 02 | P=0.919 R=0.971  TP=34 FP=3 FN=1 AP=0.981\nclass 04 | P=0.964 R=1.000  TP=27 FP=1 FN=0 AP=0.994\nclass 05 | P=0.917 R=0.880  TP=66 FP=6 FN=9 AP=0.917\nclass 06 | P=1.000 R=1.000  TP=5 FP=0 FN=0 AP=0.995\nclass 07 | P=0.743 R=0.722  TP=52 FP=18 FN=20 AP=0.690\nclass 09 | P=0.861 R=1.000  TP=31 FP=5 FN=0 AP=0.995\nclass 10 | P=1.000 R=1.000  TP=45 FP=0 FN=0 AP=0.995\nclass 11 | P=0.938 R=0.968  TP=30 FP=2 FN=1 AP=0.982\nclass 13 | P=0.928 R=0.906  TP=77 FP=6 FN=8 AP=0.926\nclass 14 | P=0.944 R=0.914  TP=85 FP=5 FN=8 AP=0.946\nclass 15 | P=1.000 R=0.968  TP=30 FP=0 FN=1 AP=0.984\nclass 16 | P=1.000 R=0.833  TP=5 FP=0 FN=1 AP=0.917\nclass 18 | P=0.971 R=0.892  TP=33 FP=1 FN=4 AP=0.943\nclass 19 | P=0.882 R=0.968  TP=30 FP=4 FN=1 AP=0.966\nclass 20 | P=0.867 R=0.867  TP=13 FP=2 FN=2 AP=0.827\nclass 21 | P=0.500 R=0.571  TP=4 FP=4 FN=3 AP=0.605\nclass 22 | P=0.500 R=1.000  TP=2 FP=2 FN=0 AP=0.995\nclass 23 | P=1.000 R=1.000  TP=48 FP=0 FN=0 AP=0.995\nclass 25 | P=0.750 R=1.000  TP=3 FP=1 FN=0 AP=0.995\nclass 27 | P=0.900 R=1.000  TP=45 FP=5 FN=0 AP=0.994\nclass 28 | P=0.941 R=0.960  TP=48 FP=3 FN=2 AP=0.972\nclass 29 | P=0.889 R=0.909  TP=40 FP=5 FN=4 AP=0.946\nclass 30 | P=0.667 R=1.000  TP=4 FP=2 FN=0 AP=0.663\nclass 31 | P=0.776 R=0.962  TP=76 FP=22 FN=3 AP=0.906\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch 83 | train loss 0.1429 | val loss 0.2600 | mAP 0.9455\nclass 00 | P=0.953 R=1.000  TP=41 FP=2 FN=0 AP=0.994\nclass 01 | P=0.944 R=0.971  TP=34 FP=2 FN=1 AP=0.980\nclass 02 | P=0.814 R=1.000  TP=35 FP=8 FN=0 AP=0.988\nclass 04 | P=1.000 R=0.963  TP=26 FP=0 FN=1 AP=0.981\nclass 05 | P=0.945 R=0.920  TP=69 FP=4 FN=6 AP=0.939\nclass 06 | P=0.833 R=1.000  TP=5 FP=1 FN=0 AP=0.995\nclass 07 | P=0.767 R=0.778  TP=56 FP=17 FN=16 AP=0.773\nclass 09 | P=0.939 R=1.000  TP=31 FP=2 FN=0 AP=0.995\nclass 10 | P=1.000 R=1.000  TP=45 FP=0 FN=0 AP=0.995\nclass 11 | P=1.000 R=0.968  TP=30 FP=0 FN=1 AP=0.984\nclass 13 | P=0.916 R=0.894  TP=76 FP=7 FN=9 AP=0.929\nclass 14 | P=0.891 R=0.882  TP=82 FP=10 FN=11 AP=0.879\nclass 15 | P=1.000 R=0.968  TP=30 FP=0 FN=1 AP=0.984\nclass 16 | P=1.000 R=0.833  TP=5 FP=0 FN=1 AP=0.917\nclass 18 | P=0.971 R=0.919  TP=34 FP=1 FN=3 AP=0.958\nclass 19 | P=0.968 R=0.968  TP=30 FP=1 FN=1 AP=0.978\nclass 20 | P=0.867 R=0.867  TP=13 FP=2 FN=2 AP=0.886\nclass 21 | P=0.833 R=0.714  TP=5 FP=1 FN=2 AP=0.810\nclass 22 | P=0.667 R=1.000  TP=2 FP=1 FN=0 AP=0.995\nclass 23 | P=0.960 R=1.000  TP=48 FP=2 FN=0 AP=0.995\nclass 25 | P=1.000 R=1.000  TP=3 FP=0 FN=0 AP=0.995\nclass 27 | P=0.957 R=1.000  TP=45 FP=2 FN=0 AP=0.995\nclass 28 | P=0.957 R=0.900  TP=45 FP=2 FN=5 AP=0.944\nclass 29 | P=0.894 R=0.955  TP=42 FP=5 FN=2 AP=0.964\nclass 30 | P=0.571 R=1.000  TP=4 FP=3 FN=0 AP=0.787\nclass 31 | P=0.826 R=0.962  TP=76 FP=16 FN=3 AP=0.943\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch 84 | train loss 0.1198 | val loss 0.2524 | mAP 0.9245\nclass 00 | P=0.976 R=1.000  TP=41 FP=1 FN=0 AP=0.995\nclass 01 | P=0.919 R=0.971  TP=34 FP=3 FN=1 AP=0.981\nclass 02 | P=0.921 R=1.000  TP=35 FP=3 FN=0 AP=0.990\nclass 04 | P=1.000 R=0.963  TP=26 FP=0 FN=1 AP=0.981\nclass 05 | P=0.945 R=0.920  TP=69 FP=4 FN=6 AP=0.941\nclass 06 | P=0.833 R=1.000  TP=5 FP=1 FN=0 AP=0.995\nclass 07 | P=0.800 R=0.778  TP=56 FP=14 FN=16 AP=0.755\nclass 09 | P=0.833 R=0.968  TP=30 FP=6 FN=1 AP=0.976\nclass 10 | P=0.918 R=1.000  TP=45 FP=4 FN=0 AP=0.995\nclass 11 | P=0.886 R=1.000  TP=31 FP=4 FN=0 AP=0.995\nclass 13 | P=0.914 R=0.871  TP=74 FP=7 FN=11 AP=0.896\nclass 14 | P=0.945 R=0.925  TP=86 FP=5 FN=7 AP=0.958\nclass 15 | P=0.886 R=1.000  TP=31 FP=4 FN=0 AP=0.990\nclass 16 | P=0.857 R=1.000  TP=6 FP=1 FN=0 AP=0.948\nclass 18 | P=1.000 R=0.892  TP=33 FP=0 FN=4 AP=0.946\nclass 19 | P=0.966 R=0.903  TP=28 FP=1 FN=3 AP=0.943\nclass 20 | P=0.867 R=0.867  TP=13 FP=2 FN=2 AP=0.813\nclass 21 | P=0.571 R=0.571  TP=4 FP=3 FN=3 AP=0.665\nclass 22 | P=0.400 R=1.000  TP=2 FP=3 FN=0 AP=0.995\nclass 23 | P=1.000 R=1.000  TP=48 FP=0 FN=0 AP=0.995\nclass 25 | P=1.000 R=1.000  TP=3 FP=0 FN=0 AP=0.995\nclass 27 | P=0.978 R=1.000  TP=45 FP=1 FN=0 AP=0.995\nclass 28 | P=0.865 R=0.900  TP=45 FP=7 FN=5 AP=0.938\nclass 29 | P=0.729 R=0.977  TP=43 FP=16 FN=1 AP=0.969\nclass 30 | P=0.190 R=1.000  TP=4 FP=17 FN=0 AP=0.524\nclass 31 | P=0.770 R=0.848  TP=67 FP=20 FN=12 AP=0.863\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch 85 | train loss 0.1091 | val loss 0.2444 | mAP 0.9452\nclass 00 | P=0.953 R=1.000  TP=41 FP=2 FN=0 AP=0.994\nclass 01 | P=0.944 R=0.971  TP=34 FP=2 FN=1 AP=0.981\nclass 02 | P=0.921 R=1.000  TP=35 FP=3 FN=0 AP=0.983\nclass 04 | P=1.000 R=0.963  TP=26 FP=0 FN=1 AP=0.981\nclass 05 | P=0.930 R=0.880  TP=66 FP=5 FN=9 AP=0.924\nclass 06 | P=0.833 R=1.000  TP=5 FP=1 FN=0 AP=0.995\nclass 07 | P=0.803 R=0.736  TP=53 FP=13 FN=19 AP=0.711\nclass 09 | P=1.000 R=1.000  TP=31 FP=0 FN=0 AP=0.995\nclass 10 | P=1.000 R=1.000  TP=45 FP=0 FN=0 AP=0.995\nclass 11 | P=0.968 R=0.968  TP=30 FP=1 FN=1 AP=0.983\nclass 13 | P=0.935 R=0.847  TP=72 FP=5 FN=13 AP=0.877\nclass 14 | P=0.784 R=0.978  TP=91 FP=25 FN=2 AP=0.961\nclass 15 | P=0.968 R=0.968  TP=30 FP=1 FN=1 AP=0.983\nclass 16 | P=1.000 R=1.000  TP=6 FP=0 FN=0 AP=0.995\nclass 18 | P=0.972 R=0.946  TP=35 FP=1 FN=2 AP=0.972\nclass 19 | P=0.938 R=0.968  TP=30 FP=2 FN=1 AP=0.976\nclass 20 | P=0.812 R=0.867  TP=13 FP=3 FN=2 AP=0.897\nclass 21 | P=0.600 R=0.429  TP=3 FP=2 FN=4 AP=0.599\nclass 22 | P=1.000 R=1.000  TP=2 FP=0 FN=0 AP=0.995\nclass 23 | P=1.000 R=1.000  TP=48 FP=0 FN=0 AP=0.995\nclass 25 | P=1.000 R=1.000  TP=3 FP=0 FN=0 AP=0.995\nclass 27 | P=0.957 R=1.000  TP=45 FP=2 FN=0 AP=0.995\nclass 28 | P=0.922 R=0.940  TP=47 FP=4 FN=3 AP=0.966\nclass 29 | P=0.878 R=0.977  TP=43 FP=6 FN=1 AP=0.982\nclass 30 | P=0.667 R=1.000  TP=4 FP=2 FN=0 AP=0.895\nclass 31 | P=0.865 R=0.975  TP=77 FP=12 FN=2 AP=0.951\n","output_type":"stream"}],"execution_count":9},{"id":"03dc7314-2557-445d-ac21-907a86ba4127","cell_type":"code","source":"# 3) Run inference once a checkpoint is available\ninference_args = argparse.Namespace(\n    data_dir=train_args.data_dir,\n    split=DatasetConfig().test_split,\n    checkpoint=Path('/kaggle/input/ecd-best-model-pth/pytorch/default/1/best_model.pth'),\n    output_dir=Path('/kaggle/working/outputs/inference'),\n    score_threshold=0.6,\n    class_threshold=[\"17=0.5\"],\n    iou_threshold=0.5,\n    max_images=200,\n    num_classes=train_args.num_classes,\n    draw_ground_truth=True,\n    seed=37,\n    pretrained_path=TrainingConfig().pretrained_weights_path,\n    fp_report=None,\n    fp_list=None,\n)\n\nrun_inference(inference_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:52:16.793662Z","iopub.execute_input":"2025-10-29T06:52:16.793920Z","iopub.status.idle":"2025-10-29T06:53:12.616731Z","shell.execute_reply.started":"2025-10-29T06:52:16.793904Z","shell.execute_reply":"2025-10-29T06:53:12.616044Z"}},"outputs":[{"name":"stderr","text":"Infer: 100%|| 200/200 [00:54<00:00,  3.70it/s]","output_type":"stream"},{"name":"stdout","text":"Inference @ IoU 0.50 | mAP 0.9521\nclass 00 | P=0.800 R=1.000  TP=8 FP=2 FN=0 AP=0.995\nclass 01 | P=0.938 R=1.000  TP=90 FP=6 FN=0 AP=0.987\nclass 02 | P=0.944 R=1.000  TP=17 FP=1 FN=0 AP=0.949\nclass 04 | P=1.000 R=0.947  TP=18 FP=0 FN=1 AP=0.974\nclass 05 | P=1.000 R=0.917  TP=11 FP=0 FN=1 AP=0.958\nclass 06 | P=0.250 R=1.000  TP=1 FP=3 FN=0 AP=0.995\nclass 07 | P=0.923 R=0.889  TP=24 FP=2 FN=3 AP=0.937\nclass 09 | P=0.909 R=1.000  TP=20 FP=2 FN=0 AP=0.957\nclass 10 | P=0.958 R=1.000  TP=23 FP=1 FN=0 AP=0.993\nclass 11 | P=0.947 R=1.000  TP=18 FP=1 FN=0 AP=0.995\nclass 13 | P=0.821 R=0.958  TP=23 FP=5 FN=1 AP=0.970\nclass 14 | P=0.952 R=0.909  TP=20 FP=1 FN=2 AP=0.911\nclass 15 | P=0.890 R=0.910  TP=81 FP=10 FN=8 AP=0.898\nclass 16 | P=0.667 R=1.000  TP=2 FP=1 FN=0 AP=0.828\nclass 17 | P=1.000 R=0.500  TP=1 FP=0 FN=1 AP=0.750\nclass 18 | P=0.879 R=0.976  TP=80 FP=11 FN=2 AP=0.958\nclass 19 | P=1.000 R=0.955  TP=84 FP=0 FN=4 AP=0.977\nclass 20 | P=1.000 R=1.000  TP=5 FP=0 FN=0 AP=0.995\nclass 21 | P=1.000 R=0.750  TP=6 FP=0 FN=2 AP=0.875\nclass 22 | P=0.800 R=1.000  TP=4 FP=1 FN=0 AP=0.995\nclass 23 | P=0.800 R=1.000  TP=24 FP=6 FN=0 AP=0.985\nclass 25 | P=1.000 R=1.000  TP=1 FP=0 FN=0 AP=0.995\nclass 27 | P=0.880 R=1.000  TP=22 FP=3 FN=0 AP=0.992\nclass 28 | P=0.840 R=0.955  TP=21 FP=4 FN=1 AP=0.960\nclass 29 | P=1.000 R=0.955  TP=21 FP=0 FN=1 AP=0.977\nclass 30 | P=0.500 R=1.000  TP=2 FP=2 FN=0 AP=0.995\nclass 31 | P=0.700 R=0.933  TP=14 FP=6 FN=1 AP=0.905\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":21}]}